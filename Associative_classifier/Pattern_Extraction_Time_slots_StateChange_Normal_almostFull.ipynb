{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "killing-blackberry",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "numerous-thursday",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Bad key \"text.kerning_factor\" on line 4 in\n",
      "/opt/anaconda3/envs/bigdatalab_cpu_202101/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test_patch.mplstyle.\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "https://github.com/matplotlib/matplotlib/blob/v3.1.2/matplotlibrc.template\n",
      "or from the matplotlib source distribution\n"
     ]
    }
   ],
   "source": [
    "#IMPORTS\n",
    "from datetime import datetime\n",
    "from pyspark.ml.fpm import PrefixSpan\n",
    "from pyspark.sql import Row\n",
    "import pyspark.sql.functions as F\n",
    "from math import sin, cos, sqrt, atan2, radians \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import operator\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "recorded-sucking",
   "metadata": {},
   "outputs": [],
   "source": [
    "#parameters\n",
    "interval=30 #time window\n",
    "th=1 #distance\n",
    "window_size=3 #window size\n",
    "maxDelta=3 #how many delta\n",
    "support=0\n",
    "support_str=\"0\"\n",
    "station_status= \"StateChange_Normal_almostFull\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "cathedral-heath",
   "metadata": {},
   "outputs": [],
   "source": [
    "#FILES\n",
    "#Considero soltanto le stazioni di San Francisco\n",
    "inputPath  = \"file:///home/bigdata-01QYD/s270240/bike_sharing/Associative_classifier/Datasets/SanFrancisco_status_train.csv\"\n",
    "STATION_PATH=\"file:///home/bigdata-01QYD/s270240/bike_sharing/station.csv\"\n",
    "#save file of first filter\n",
    "output_file=f'./Results_extraction/{interval}min_{int(th*1000)}m/{results_{station_status}_{interval}_{int(th*1000)}_{support_str}({window_size}-{maxDelta})_ordered_by_confidence.txt'\n",
    "output_file_conf1=f'./Results_extraction/{interval}min_{int(th*1000)}m/Time_slots/results_6-10_{station_status}_{interval}_{int(th*1000)}_{support_str}({window_size}-{maxDelta})_ordered_by_confidence.txt'\n",
    "output_file_conf2=f'./Results_extraction/{interval}min_{int(th*1000)}m/Time_slots/results_10-14_{station_status}_{interval}_{int(th*1000)}_{support_str}({window_size}-{maxDelta})_ordered_by_confidence.txt'\n",
    "output_file_conf3=f'./Results_extraction/{interval}min_{int(th*1000)}m/Time_slots/results_14-17_{station_status}_{interval}_{int(th*1000)}_{support_str}({window_size}-{maxDelta})_ordered_by_confidence.txt'\n",
    "output_file_conf4=f'./Results_extraction/{interval}min_{int(th*1000)}m/Time_slots/results_17-20_{station_status}_{interval}_{int(th*1000)}_{support_str}({window_size}-{maxDelta})_ordered_by_confidence.txt'\n",
    "output_file_conf5=f'./Results_extraction/{interval}min_{int(th*1000)}m/Time_slots/results_20-24_{station_status}_{interval}_{int(th*1000)}_{support_str}({window_size}-{maxDelta})_ordered_by_confidence.txt'\n",
    "output_file_conf6=f'./Results_extraction/{interval}min_{int(th*1000)}m/Time_slots/results_0-6_{station_status}_{interval}_{int(th*1000)}_{support_str}({window_size}-{maxDelta})_ordered_by_confidence.txt'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "final-objective",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputDF = spark.read.format(\"csv\")\\\n",
    ".option(\"delimiter\", \",\")\\\n",
    ".option(\"header\", True)\\\n",
    ".option(\"inferSchema\", True).load(inputPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "nuclear-trademark",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputDF=inputDF.filter(\"docks_available is not null\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "motivated-horizontal",
   "metadata": {},
   "outputs": [],
   "source": [
    "#filter for these fields\n",
    "filteredDF = inputDF.filter(\"docks_available<>0 OR bikes_available<>0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ideal-nashville",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SFancisco_stations = [41, 42, 45, 46, 47, 48, 49, 50, 51, 39, 54, 55, 56, 57, 58, 59, 60, \n",
    "#                       61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 82]\n",
    "# filteredDF = filteredDF[filteredDF['station_id'].isin(SFancisco_stations)]\n",
    "# filteredDF.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "blank-progress",
   "metadata": {},
   "outputs": [],
   "source": [
    "#determine if the station is normal or almost full\n",
    "def stateFunction(docks_available,bikes_available):\n",
    "    if docks_available<=2: # almostFull\n",
    "        return 1\n",
    "    else:         #normal\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "revolutionary-blanket",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.stateFunction(docks_available, bikes_available)>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.udf.register(\"state\", stateFunction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "literary-senegal",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getInt(station):\n",
    "    return (station)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "geographic-laundry",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.getInt(station)>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.udf.register(\"intValue\", getInt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "falling-superintendent",
   "metadata": {},
   "outputs": [],
   "source": [
    "getStatusDF = filteredDF.selectExpr(\"station_id\",\"time\", \"state(docks_available,bikes_available) as status\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "postal-attitude",
   "metadata": {},
   "outputs": [],
   "source": [
    "# getStatusDF.show(100,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "adopted-superior",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a view\n",
    "getStatusDF.createOrReplaceTempView(\"readings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "modular-calendar",
   "metadata": {},
   "outputs": [],
   "source": [
    "#select station, year, month, day, hour, minute, status ordered by time\n",
    "ss=spark.sql(\"\"\"SELECT  station_id , YEAR(time) as year, MONTH(time) as month, DAY(time) as day, HOUR(time)as hour, MINUTE(time) as minute, status\n",
    "FROM readings\n",
    "GROUP BY station_id, year, month, day,hour,minute, status\n",
    "ORDER BY  station_id,year, month,day, hour,minute\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "empirical-bahamas",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create rdd and group into interval\n",
    "my_rdd=ss.rdd.map(tuple)\n",
    "rdd=my_rdd.map(lambda line: (line[0],line[1],line[2], line[3], line[4], int(line[5]/interval), line[6])).distinct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "monetary-calcium",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rdd.collect()\n",
    "\n",
    "# [(6, 2013, 12, 31, 1, 1, '2'),\n",
    "#  (6, 2014, 1, 14, 22, 1, '2'),\n",
    "#  (6, 2014, 1, 16, 0, 1, '2'),\n",
    "#  (6, 2014, 1, 19, 6, 0, '2'),\n",
    "#  (6, 2014, 1, 23, 21, 1, '2'),\n",
    "#  (6, 2014, 1, 26, 10, 1, '2'),\n",
    "#  (6, 2014, 1, 30, 14, 1, '2'), ... ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "reduced-presence",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get distinct stations to calculate distances\n",
    "id_stations=rdd.map(lambda line: line[0]).distinct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fancy-score",
   "metadata": {},
   "outputs": [],
   "source": [
    "tot_id_stations=id_stations.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fabulous-prison",
   "metadata": {},
   "outputs": [],
   "source": [
    "#obtain timestamp and info\n",
    "def getMap2(line):\n",
    "    id_station=str(line[0])\n",
    "    year=int(line[1])\n",
    "    month=int(line[2])\n",
    "    day=int(line[3])\n",
    "    hour=int(line[4])\n",
    "    minute=int(line[5])   \n",
    "    timestamp= datetime(year,month, day, hour, minute)  \n",
    "    status=int(line[6])\n",
    "    info=str(id_station)+' '+str(status)\n",
    "    return (timestamp, info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "wound-listening",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_map2=rdd.map(getMap2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "infrared-bathroom",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_map2.collect()\n",
    "\n",
    "# [(datetime.datetime(2014, 3, 31, 18, 1), '11 2'),\n",
    "#  (datetime.datetime(2014, 4, 6, 21, 0), '11 2'),\n",
    "#  (datetime.datetime(2014, 4, 10, 22, 1), '11 2'),\n",
    "#  (datetime.datetime(2014, 4, 12, 8, 1), '11 2'),\n",
    "#  (datetime.datetime(2014, 4, 13, 8, 0), '11 2'), ... ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "alternate-calibration",
   "metadata": {},
   "outputs": [],
   "source": [
    "reduceK2=get_map2.reduceByKey(lambda l1,l2 :(l1+','+l2)).sortByKey()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "subjective-component",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reduceK2.collect()\n",
    "\n",
    "# [(datetime.datetime(2013, 8, 29, 10, 0), '59 0,67 0,4 1,13 1,67 1,6 0,2 0,7 1,16 0,70 1,70 0'),\n",
    "#  (datetime.datetime(2013, 8, 29, 10, 1), '67 0,7 1,16 0,59 0,13 1,67 1,70 0,2 0,7 0,69 0,4 1,13 0,73 0'),\n",
    "#  (datetime.datetime(2013, 8, 29, 11, 0), '67 1,2 0,4 0,73 0,67 0,69 0,16 0,70 0,4 1'),\n",
    "#  (datetime.datetime(2013, 8, 29, 11, 1), '16 0,73 0,2 0,69 0,67 0'),\n",
    "#  (datetime.datetime(2013, 8, 29, 12, 0), '69 1,69 0,16 0,2 0,73 0'), ... ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "steady-istanbul",
   "metadata": {},
   "outputs": [],
   "source": [
    "#divide rdd in different time slots\n",
    "slot1= reduceK2.filter(lambda line : line[0].hour >=6 and line[0].hour<10)\n",
    "slot2= reduceK2.filter(lambda line : line[0].hour >=10 and line[0].hour<14)\n",
    "slot3= reduceK2.filter(lambda line : line[0].hour >=14 and line[0].hour<17)\n",
    "slot4= reduceK2.filter(lambda line : line[0].hour >=17 and line[0].hour<20)\n",
    "slot5= reduceK2.filter(lambda line : line[0].hour >=20 and line[0].hour<=23)\n",
    "slot6= reduceK2.filter(lambda line : line[0].hour >=0 and line[0].hour<6)\n",
    "\n",
    "#create a list contaning all slots rdd\n",
    "slots = [slot1, slot2, slot3, slot4, slot5, slot6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "comparable-oasis",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_tots = []\n",
    "for i, slot in enumerate(slots):\n",
    "    list_tots.append(slot.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "growing-staff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    }
   ],
   "source": [
    "print(len(list_tots))\n",
    "# list_tot[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "increased-cocktail",
   "metadata": {},
   "outputs": [],
   "source": [
    "multiple_list_filtered = []\n",
    "\n",
    "for list_tot in list_tots:\n",
    "\n",
    "    voc_status={}\n",
    "    voc_timestamp={}\n",
    "    list_filtered=[]\n",
    "\n",
    "    voc_tmp={}\n",
    "\n",
    "    # previous_list=[]\n",
    "    # updated_list=[]\n",
    "\n",
    "    for i, el in enumerate(list_tot):\n",
    "        #updated_list=[]    \n",
    "        information=el[1]   \n",
    "        tmp_list_station='' \n",
    "        voc_tmp={}\n",
    "\n",
    "        for info in information.split(','):\n",
    "    #         print()\n",
    "    #         print(info)\n",
    "            station=str(info.split(' ')[0])\n",
    "\n",
    "            status=int(info.split(' ')[1])\n",
    "            if  station not in voc_status:\n",
    "    #             print (f\"station: {station} NOT PRESENT\")\n",
    "                voc_status[station]=status\n",
    "                tmp_time= el[0].replace(minute=el[0].minute*interval) #riconverto nei minuti originali \n",
    "                voc_timestamp[station]=tmp_time\n",
    "    #             print (f\"actual_time: {voc_timestamp[station]}\")\n",
    "                stringa=''\n",
    "                if status==1:\n",
    "                    stringa='QuasiPiena'\n",
    "                else:\n",
    "                    stringa='Normal'    \n",
    "                tmp_list_station+=','+station+'_'+ stringa \n",
    "    #             print(tmp_list_station)\n",
    "                #updated_list.append(station)\n",
    "\n",
    "            else:\n",
    "    #             print (f\"station: {station} PRESENT\")\n",
    "                times=voc_timestamp[station]\n",
    "    #             print (f\"times: {times}\")\n",
    "                tmp_time= el[0].replace(minute=el[0].minute*interval) #riconverto nei minuti originali\n",
    "                actual_time=tmp_time\n",
    "    #             print (f\"actual_time: {actual_time}\")\n",
    "    #             print (f\"old satus: {voc_status[station]}\")\n",
    "    #             print (f\"new satus: {status}\")\n",
    "\n",
    "                if times == actual_time:    #inside same timeslot\n",
    "                    if status != voc_status[station]:   # if status changed   \n",
    "    #                     print(\"STATUS CHANGED INSIDE THE SAME TIMESLOT!!!!!\")\n",
    "                        voc_status[station]=3  #set flag of status changed inside same timeslot\n",
    "                        stringa=''\n",
    "                        if status==1:\n",
    "                            stringa='QuasiPiena'\n",
    "                        else:\n",
    "                            stringa='Normal'    \n",
    "                        tmp_list_station+=','+station+'_'+ stringa \n",
    "    #                     print(tmp_list_station)\n",
    "\n",
    "                else:            # different timeslot\n",
    "    #                 print(f\"actual_time.minute-times.minute: {int((actual_time-times).total_seconds() / 60.0)}\")\n",
    "    #                 if ((actual_time.hour == times.hour and actual_time.minute-times.minute==1) or\n",
    "    #                     (actual_time.hour-times.hour==1 and actual_time.minute-times.minute==-1)):\n",
    "\n",
    "                    if (int((actual_time-times).total_seconds() / 60.0)== interval): # after less than 30 minutes\n",
    "    #                     print(f\"after <= 30 minutes\")\n",
    "                        if voc_status[station]==3:         #if status changed inside the same old timeslot        \n",
    "    #                         print(\"STATUS CHANGED INSIDE THE SAME OLD TIMESLOT!!!!!\")\n",
    "                            voc_status[station]=status     #update old status with new one\n",
    "                            voc_timestamp[station]=actual_time      #update old timestamp with new one\n",
    "                            stringa=''\n",
    "                            if status==1:\n",
    "                                stringa='QuasiPiena'\n",
    "                            else:\n",
    "                                stringa='Normal'\n",
    "                            tmp_list_station+=','+station+'_'+ stringa \n",
    "    #                         print(tmp_list_station)\n",
    "                            #updated_list.append(station)\n",
    "\n",
    "                        elif voc_status[station] != status:  # if status changed between old and new timeslot \n",
    "    #                         print(\"STATUS CHANGED!!!!!\")\n",
    "                            voc_status[station]=status     #update old status with new one\n",
    "                            voc_timestamp[station]=actual_time    #update old timestamp with new one\n",
    "                            stringa=''\n",
    "                            if status==1:\n",
    "                                stringa='QuasiPiena'\n",
    "                            else :\n",
    "                                stringa='Normal'\n",
    "                            tmp_list_station+=','+station+'_'+ stringa \n",
    "    #                         print(tmp_list_station)\n",
    "\n",
    "                            if station in voc_tmp:\n",
    "                                if voc_tmp[station]==1:\n",
    "                                     tmp_list_station+=','+station+'_QuasiPiena'\n",
    "                                elif voc_tmp[station]==0:\n",
    "                                     tmp_list_station+=','+station+'_Normal'\n",
    "\n",
    "    #                             print(tmp_list_station)\n",
    "\n",
    "\n",
    "                        elif  voc_status[station] == status: # if status did not change between old and new timeslot\n",
    "    #                         print(\"STATUS DID NOT CHANGE!!!!!\")\n",
    "                            voc_tmp[station]=status\n",
    "                            tmp_list_station+=','+station+'_NoChange'\n",
    "    #                         print(tmp_list_station)\n",
    "    #                         voc_timestamp[station]=actual_time     #!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "\n",
    "                    # after more than 30 minutes        \n",
    "                    else:\n",
    "    #                     print(f\"after >= 30 minutes\")\n",
    "                        if voc_status[station] != status:\n",
    "                            stringa=''\n",
    "                            if status==1:\n",
    "                                stringa='QuasiPiena'\n",
    "                            else :\n",
    "                                stringa='Normal'\n",
    "                            tmp_list_station+=','+station+'_'+ stringa \n",
    "    #                         print(tmp_list_station)\n",
    "                        else:\n",
    "                            tmp_list_station+=','+station+'_NoChange'\n",
    "    #                     print (f\"REMOVING STATION {station}\")    \n",
    "    #                     del voc_status[station]\n",
    "    #                     del voc_timestamp[station]\n",
    "                        voc_timestamp[station]=actual_time\n",
    "\n",
    "\n",
    "        list_filtered.append((el[0],tmp_list_station[1::]))\n",
    "    \n",
    "    multiple_list_filtered.append(list_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "particular-break",
   "metadata": {},
   "outputs": [],
   "source": [
    "multiple_reduceK = []\n",
    "\n",
    "for list_filtered in multiple_list_filtered:\n",
    "    multiple_reduceK.append(spark.sparkContext.parallelize(list_filtered))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "awful-regular",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    }
   ],
   "source": [
    "print(len(multiple_reduceK))\n",
    "# reduceK.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "sweet-scheme",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_df_list = []\n",
    "\n",
    "for reduceK in multiple_reduceK:\n",
    "    my_df_list.append(reduceK.toDF())\n",
    "#my_df.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "coupled-roads",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, my_df in enumerate(my_df_list):\n",
    "    my_df.createOrReplaceTempView(f\"view{i}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "european-sixth",
   "metadata": {},
   "outputs": [],
   "source": [
    "add_rows_list = []\n",
    "for i in range(0,len(my_df_list)):\n",
    "    s2 = spark.sql(f\"\"\"SELECT ROW_NUMBER() OVER(ORDER BY _1,_2) as id ,_1, _2\n",
    "                FROM view{i} \"\"\")\n",
    "    add_rows_list.append(s2)\n",
    "\n",
    "\n",
    "# s2=spark.sql(\"\"\"SELECT ROW_NUMBER() OVER(ORDER BY _1,_2) as id ,_1, _2\n",
    "# FROM view \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "passing-universal",
   "metadata": {},
   "outputs": [],
   "source": [
    "#identifier of the timestamp, info\n",
    "rdd_scheme_list = []\n",
    "for s2 in add_rows_list:\n",
    "    rdd_scheme=s2.rdd.map(tuple).map(lambda line: (line[0], line[2]))\n",
    "    rdd_scheme_list.append(rdd_scheme)\n",
    "\n",
    "\n",
    "# rdd_scheme=s2.rdd.map(tuple).map(lambda line: (line[0], line[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "american-lying",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rdd_scheme.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "dominant-penetration",
   "metadata": {},
   "outputs": [],
   "source": [
    "#obtain window, station-status\n",
    "def giveSplit(line):   \n",
    "    id_window=( int(line[0] ))\n",
    "    lista=[]    \n",
    "    counter=id_window    \n",
    "    while counter>=1:\n",
    "        lista.append(('Window '+str(counter),(line[1])))\n",
    "        counter=counter-1\n",
    "        if (id_window-counter)==window_size:\n",
    "            return lista  \n",
    "    return lista"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "allied-building",
   "metadata": {},
   "outputs": [],
   "source": [
    "mapData_list = []\n",
    "\n",
    "for rdd_scheme in rdd_scheme_list:\n",
    "    mapData=rdd_scheme.flatMap(giveSplit)\n",
    "    mapData_list.append(mapData)\n",
    "\n",
    "# mapData=rdd_scheme.flatMap(giveSplit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "handy-force",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mapData.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "overall-congress",
   "metadata": {},
   "outputs": [],
   "source": [
    "filteredD_list = []\n",
    "for mapData in mapData_list:\n",
    "    filteredD=mapData.filter(lambda line: line[1]!='')\n",
    "    filteredD_list.append(filteredD)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "pressing-basic",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filteredD.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "united-catalog",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each window get all info\n",
    "all_keys_list = []\n",
    "\n",
    "for filteredD in filteredD_list:\n",
    "    all_keys=filteredD.reduceByKey(lambda l1,l2:(l1+'-'+l2))    \n",
    "    all_keys_list.append(all_keys)\n",
    "\n",
    "\n",
    "# all_keys=filteredD.reduceByKey(lambda l1,l2:(l1+'-'+l2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "spoken-premises",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_keys.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "historical-singles",
   "metadata": {},
   "outputs": [],
   "source": [
    "#finestra temporale\n",
    "def reduceKeys(line):   \n",
    "    lista=[]\n",
    "    #lista.append(line[0])\n",
    "    line_split=line[1].split(\"-\")\n",
    "    #return line_split[0]\n",
    "    count=len(line_split)\n",
    "    tot=[]\n",
    "    for val in range(count):\n",
    "        li=[]\n",
    "        stations=line_split[val].split(',')\n",
    "        for st in stations:\n",
    "            all_string_st=st.split('_')[0]+'_'+'T'+str(val)+'_'+st.split('_')[1]\n",
    "            li.append(all_string_st)\n",
    "        tot.append(li)\n",
    "    lista.append((line[0],(tot))) \n",
    "    return lista"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "caring-preliminary",
   "metadata": {},
   "outputs": [],
   "source": [
    "windows_list = []\n",
    "\n",
    "for all_keys in all_keys_list:\n",
    "    windows=all_keys.flatMap(reduceKeys)\n",
    "    windows_list.append(windows)\n",
    "\n",
    "# windows=all_keys.flatMap(reduceKeys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "passing-junction",
   "metadata": {},
   "outputs": [],
   "source": [
    "# windows_list[5].collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "english-reference",
   "metadata": {},
   "outputs": [],
   "source": [
    "#filtering NoChange events\n",
    "def filterNormal(line):\n",
    "    complete_list = []\n",
    "    for list_el in line[1]:\n",
    "        event_list = copy.deepcopy(list_el)\n",
    "        for el in list_el:\n",
    "            if \"NoChange\"in el:\n",
    "                event_list.remove(el)\n",
    "#         new_event_list = \",\".join(event_list)\n",
    "        complete_list.append(event_list)\n",
    "    return (line[0], complete_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "operational-monday",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove normal and noChange events\n",
    "windows_withoutNormal_list = []\n",
    "\n",
    "for windows in windows_list:\n",
    "    windows_withoutNormal = windows.map(filterNormal)\n",
    "    windows_withoutNormal_list.append(windows_withoutNormal)\n",
    "    \n",
    "# windows_withoutNormal = windows.map(filterNormal)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "continuous-surveillance",
   "metadata": {},
   "outputs": [],
   "source": [
    "# windows_withoutNormal.collect()\n",
    "\n",
    "# [('Window 1',\n",
    "#   [[],\n",
    "#    ['60_T1_QuasiPiena', '64_T1_QuasiPiena', '45_T1_Piena', '45_T1_QuasiPiena'],\n",
    "#    ['45_T2_Piena', '64_T2_Piena', '64_T2_QuasiPiena', '45_T2_QuasiPiena']]),\n",
    "#  ('Window 2',\n",
    "#   [['60_T0_QuasiPiena', '64_T0_QuasiPiena', '45_T0_Piena', '45_T0_QuasiPiena'],\n",
    "#    ['45_T1_Piena', '64_T1_Piena', '64_T1_QuasiPiena', '45_T1_QuasiPiena'],\n",
    "#    ['64_T2_QuasiPiena', '35_T2_QuasiPiena']]),... ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "commercial-machine",
   "metadata": {},
   "outputs": [],
   "source": [
    "#filtering empty list ''\n",
    "def filterEmptyList(line):\n",
    "    filtered_list = list(filter(None, line[1]))\n",
    "    return (line[0], filtered_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "spare-perfume",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_windows_list =[]\n",
    "\n",
    "for windows_withoutNormal in windows_withoutNormal_list:\n",
    "    filtered_windows = windows_withoutNormal.map(filterEmptyList)\n",
    "    filtered_windows_list.append(filtered_windows)\n",
    "\n",
    "\n",
    "# filtered_windows = windows_withoutNormal.map(filterEmptyList)\n",
    "# filtered_windows.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "united-orchestra",
   "metadata": {},
   "outputs": [],
   "source": [
    "#filtering empty window\n",
    "filtered_windows2_list = []\n",
    "\n",
    "for filtered_windows in filtered_windows_list:\n",
    "    filtered_windows2 = filtered_windows.filter(lambda line: line[1])\n",
    "    filtered_windows2_list.append(filtered_windows2)\n",
    "\n",
    "\n",
    "# filtered_windows2 = filtered_windows.filter(lambda line: line[1])\n",
    "# filtered_windows2.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "unavailable-timeline",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save station file\n",
    "stationsDF = spark.read.format(\"csv\")\\\n",
    ".option(\"delimiter\", \",\")\\\n",
    ".option(\"header\", True)\\\n",
    ".option(\"inferSchema\", True).load(STATION_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "invisible-jewelry",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get only rows interested: only the used stations \n",
    "necessary_rows=stationsDF.filter(F.col(\"id\").isin(tot_id_stations)).sort(\"id\").rdd.map(tuple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "regulation-brass",
   "metadata": {},
   "outputs": [],
   "source": [
    "#necessary_rows.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "billion-agreement",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get info of stations about coordinates and name\n",
    "coordinates=necessary_rows.map(lambda line: (line[0],(str(line[2])+','+str(line[3]))))\n",
    "names_stations=necessary_rows.map(lambda line: (line[0],line[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "obvious-mother",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_coo=coordinates.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "reserved-gibson",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list_coo\n",
    "\n",
    "# [(2, '37.329732,-121.90178200000001'),\n",
    "#  (3, '37.330698,-121.888979'),\n",
    "#  (4, '37.333988,-121.894902'),\n",
    "#  (5, '37.331415,-121.8932'),\n",
    "#  (6, '37.336721000000004,-121.894074'), ...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "inside-deadline",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a dictionary in which the key is the station and value is the info about coordinates\n",
    "dic_co=coordinates.collectAsMap()\n",
    "dic_coordinates=sc.broadcast(dic_co)\n",
    "#dic_coordinates.value[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "seven-conversion",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to retrieve distance between 2 stations\n",
    "def getDistance(station1,station2):\n",
    "    # approximate radius of earth in km\n",
    "    R = 6373.0    \n",
    "    lat_a=float(station1.split(',')[0])\n",
    "    lat_b=float(station2.split(',')[0])\n",
    "    long_a=float(station1.split(',')[1])\n",
    "    long_b=float(station2.split(',')[1])\n",
    "    \n",
    "    lat1=radians(lat_a)\n",
    "    lat2=radians(lat_b)\n",
    "    lon1=radians(long_a)\n",
    "    lon2=radians(long_b)\n",
    "    \n",
    "    dlon = lon2 - lon1\n",
    "    dlat = lat2 - lat1\n",
    "\n",
    "    a = sin(dlat / 2)**2 + cos(lat1) * cos(lat2) * sin(dlon / 2)**2\n",
    "    c = 2 * atan2(sqrt(a), sqrt(1 - a))\n",
    "\n",
    "    distance = R * c\n",
    "    return distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "blond-state",
   "metadata": {},
   "outputs": [],
   "source": [
    "#voc in which the key is a pair of stations and value is the distance\n",
    "voc_distances={}\n",
    "for i in range(len(list_coo)):\n",
    "    for j in range(i+1,len(list_coo)):\n",
    "        station1=list_coo[i][0]\n",
    "        station2=list_coo[j][0]\n",
    "        d_i=list_coo[i][1]\n",
    "        d_j=list_coo[j][1]\n",
    "        distance=getDistance(d_i,d_j)\n",
    "        id_stations=str(station1)+' '+str(station2)\n",
    "        voc_distances[id_stations]=distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "willing-workplace",
   "metadata": {},
   "outputs": [],
   "source": [
    "# voc_distances['2 3']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "technical-sound",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Applicazione “Delta Spaziale”\n",
    "def giveSpatialWindow(line):\n",
    "    lista=[]    \n",
    "    time0=line[1][0]    #\n",
    "    dic={}\n",
    "    \n",
    "    count_windows=len(line[1])#tot windows\n",
    "\n",
    "    for station in time0:# only first window\n",
    "        act_station=int(station.split('_')[0])\n",
    "        #lista_station=[] \n",
    "        list_tmp=[]\n",
    "        \n",
    "        #for each window\n",
    "        for i,window in enumerate(line[1]):           \n",
    "            second_lista=[]\n",
    "            #for each element of a window\n",
    "            for all_el in window :\n",
    "                #second_lista=[]\n",
    "                \n",
    "                act_all_el=int(all_el.split('_')[0])\n",
    "                time_window= all_el.split('_')[1]\n",
    "                state=all_el.split('_')[2]\n",
    "               \n",
    "                if act_station!=act_all_el:\n",
    "                    \n",
    "                    key=''\n",
    "                    if act_station<act_all_el:\n",
    "                        key=str(act_station)+' '+str(act_all_el)\n",
    "                    else:\n",
    "                        key=str(act_all_el)+' '+str(act_station)                    \n",
    "                    \n",
    "                    dist=voc_distances[key]\n",
    "                    if dist<=maxDelta*th:\n",
    "                        delta=0\n",
    "                        for d in range(1,maxDelta+1):\n",
    "                            if d*th>=dist:\n",
    "                                delta=d\n",
    "                                break                        \n",
    "                        string=state+'_'+time_window+'_'+str(delta)\n",
    "                        second_lista.append(string)\n",
    "                else:\n",
    "                    string=state+'_'+time_window+'_'+str(0)\n",
    "                    second_lista.append(string)\n",
    "                    \n",
    "            if len(second_lista)>0:\n",
    "                list_tmp.append(second_lista)\n",
    "        lista.append(((line[0]+'|'+str(act_station)),list_tmp))\n",
    "    \n",
    "    return lista"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "built-reference",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num elements in slot1: 18195\n",
      "num elements in slot2: 13445\n",
      "num elements in slot3: 13784\n",
      "num elements in slot4: 11892\n",
      "num elements in slot5: 9501\n",
      "num elements in slot6: 11720\n"
     ]
    }
   ],
   "source": [
    "spatial_app_list = []\n",
    "\n",
    "for i, filtered_windows2 in enumerate(filtered_windows2_list):\n",
    "    spatial_app=filtered_windows2.flatMap(giveSpatialWindow)\n",
    "    print(f\"num elements in slot{i+1}: {spatial_app.count()}\")\n",
    "    spatial_app_list.append(spatial_app)\n",
    "\n",
    "# spatial_app=filtered_windows2.flatMap(giveSpatialWindow)\n",
    "# print(spatial_app.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "electric-european",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spatial_app.collect()\n",
    "\n",
    "# [('Window 1|59',\n",
    "#   [['QuasiVuota_T0_0', 'QuasiVuota_T0_1', 'Vuota_T0_1', 'Vuota_T0_3', 'QuasiVuota_T0_3'],\n",
    "#    ['QuasiVuota_T1_1', 'Vuota_T1_1', 'QuasiVuota_T1_3', 'QuasiVuota_T1_3', 'QuasiVuota_T1_3']]),\n",
    "#  ('Window 1|67',\n",
    "#   [['QuasiVuota_T0_1', 'QuasiVuota_T0_0', 'Vuota_T0_0', 'Vuota_T0_2', 'QuasiVuota_T0_2'],\n",
    "#    ['QuasiVuota_T1_0', 'Vuota_T1_0', 'QuasiVuota_T1_2', 'QuasiVuota_T1_2', 'QuasiVuota_T1_3']]), ...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "distinct-ancient",
   "metadata": {},
   "outputs": [],
   "source": [
    "def row_seq(line):\n",
    "    true=line[1]\n",
    "    string=Row(sequence=true)\n",
    "    return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "expected-round",
   "metadata": {},
   "outputs": [],
   "source": [
    "spatial_list = []\n",
    "\n",
    "for spatial_app in spatial_app_list:\n",
    "    spatial=spatial_app.map(row_seq)\n",
    "    spatial_list.append(spatial)\n",
    "\n",
    "# spatial=spatial_app.map(row_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "personal-listening",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spatial.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "posted-herald",
   "metadata": {},
   "outputs": [],
   "source": [
    "# seq_rdd =spatial.rdd.map(tuple)\n",
    "filtered_seq_list = []\n",
    "\n",
    "for spatial in spatial_list:\n",
    "    filtered_seq = spatial.filter(lambda line: \"T0\" in line[0][0][0])\n",
    "    filtered_seq_list.append(filtered_seq)\n",
    "\n",
    "\n",
    "# filtered_seq = spatial.filter(lambda line: \"T0\" in line[0][0][0])\n",
    "# filtered_seq.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "involved-administration",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num elements in df1: 17453\n",
      "num elements in df2: 12336\n",
      "num elements in df3: 13377\n",
      "num elements in df4: 10980\n",
      "num elements in df5: 6965\n",
      "num elements in df6: 9597\n"
     ]
    }
   ],
   "source": [
    "#create dataframes\n",
    "df_list = []\n",
    "\n",
    "for i, filtered_seq in enumerate(filtered_seq_list):\n",
    "    df=filtered_seq.toDF()\n",
    "    print (f\"num elements in df{i+1}: {df.count()}\")\n",
    "    df_list.append(df)\n",
    "\n",
    "# df=filtered_seq.toDF()\n",
    "# len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "african-narrative",
   "metadata": {},
   "outputs": [],
   "source": [
    "supports=[support]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "characteristic-danger",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|sequence                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |\n",
      "+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|[[Normal_T0_0, Normal_T0_1, Normal_T0_3, Normal_T0_3, Normal_T0_2, Normal_T0_3, Normal_T0_3, Normal_T0_3, Normal_T0_3, Normal_T0_2, Normal_T0_3, Normal_T0_2, Normal_T0_2, QuasiPiena_T0_3, Normal_T0_3, Normal_T0_3, QuasiPiena_T0_1, Normal_T0_1, QuasiPiena_T0_2, Normal_T0_3, Normal_T0_3, Normal_T0_2, Normal_T0_3, Normal_T0_3, Normal_T0_3, Normal_T0_2, Normal_T0_3, Normal_T0_2, Normal_T0_3, QuasiPiena_T0_1, Normal_T0_1, Normal_T0_1, Normal_T0_3, Normal_T0_2, Normal_T0_2], [Normal_T1_1, Normal_T1_3, QuasiPiena_T1_3], [QuasiPiena_T2_3, Normal_T2_1]]|\n",
      "|[[Normal_T0_1, Normal_T0_0, Normal_T0_3, Normal_T0_3, Normal_T0_2, Normal_T0_3, Normal_T0_3, Normal_T0_3, Normal_T0_3, Normal_T0_2, Normal_T0_3, Normal_T0_2, Normal_T0_2, QuasiPiena_T0_3, Normal_T0_3, Normal_T0_3, QuasiPiena_T0_1, Normal_T0_1, QuasiPiena_T0_3, Normal_T0_3, Normal_T0_3, Normal_T0_2, Normal_T0_3, Normal_T0_3, Normal_T0_3, Normal_T0_2, Normal_T0_3, Normal_T0_2, QuasiPiena_T0_1, Normal_T0_1, Normal_T0_2, Normal_T0_3, Normal_T0_3, Normal_T0_2], [Normal_T1_1, Normal_T1_3, QuasiPiena_T1_3], [QuasiPiena_T2_3, Normal_T2_1]]             |\n",
      "|[[Normal_T0_3, Normal_T0_3, Normal_T0_0, Normal_T0_1, Normal_T0_2, Normal_T0_1, Normal_T0_1, Normal_T0_2, Normal_T0_1, Normal_T0_3, Normal_T0_1, Normal_T0_1, Normal_T0_2, QuasiPiena_T0_2, Normal_T0_1, Normal_T0_1, QuasiPiena_T0_3, QuasiPiena_T0_1, Normal_T0_2, Normal_T0_1, Normal_T0_2, Normal_T0_1, Normal_T0_1, Normal_T0_2, Normal_T0_1, Normal_T0_2, Normal_T0_2, Normal_T0_2, Normal_T0_3, Normal_T0_2, Normal_T0_1, Normal_T0_1, Normal_T0_1], [Normal_T1_2, QuasiPiena_T1_2], [QuasiPiena_T2_2, Normal_T2_3]]                                           |\n",
      "|[[Normal_T0_3, Normal_T0_3, Normal_T0_1, Normal_T0_0, Normal_T0_2, Normal_T0_1, Normal_T0_1, Normal_T0_2, Normal_T0_1, Normal_T0_3, Normal_T0_1, Normal_T0_2, Normal_T0_2, QuasiPiena_T0_2, Normal_T0_1, Normal_T0_1, QuasiPiena_T0_3, QuasiPiena_T0_1, Normal_T0_3, Normal_T0_2, Normal_T0_2, Normal_T0_1, Normal_T0_1, Normal_T0_3, Normal_T0_1, Normal_T0_2, Normal_T0_2, Normal_T0_2, Normal_T0_3, Normal_T0_2, Normal_T0_2, Normal_T0_1, Normal_T0_1], [Normal_T1_2, QuasiPiena_T1_2], [QuasiPiena_T2_2, Normal_T2_3]]                                           |\n",
      "|[[Normal_T0_2, Normal_T0_2, Normal_T0_2, Normal_T0_2, Normal_T0_0, Normal_T0_2, Normal_T0_2, Normal_T0_2, Normal_T0_2, Normal_T0_2, Normal_T0_1, Normal_T0_1, Normal_T0_1, QuasiPiena_T0_2, Normal_T0_2, Normal_T0_2, QuasiPiena_T0_1, Normal_T0_2, QuasiPiena_T0_1, Normal_T0_2, Normal_T0_2, Normal_T0_1, Normal_T0_2, Normal_T0_2, Normal_T0_2, Normal_T0_1, Normal_T0_2, Normal_T0_1, Normal_T0_2, QuasiPiena_T0_2, Normal_T0_2, Normal_T0_1, Normal_T0_2, Normal_T0_1, Normal_T0_1], [Normal_T1_2, Normal_T1_2, QuasiPiena_T1_2], [QuasiPiena_T2_2, Normal_T2_1]]|\n",
      "|[[Normal_T0_3, Normal_T0_3, Normal_T0_1, Normal_T0_1, Normal_T0_2, Normal_T0_0, Normal_T0_1, Normal_T0_2, Normal_T0_1, Normal_T0_3, Normal_T0_1, Normal_T0_1, Normal_T0_1, QuasiPiena_T0_2, Normal_T0_1, Normal_T0_1, QuasiPiena_T0_2, Normal_T0_3, QuasiPiena_T0_1, Normal_T0_2, Normal_T0_1, Normal_T0_1, Normal_T0_1, Normal_T0_1, Normal_T0_2, Normal_T0_1, Normal_T0_2, Normal_T0_2, Normal_T0_2, QuasiPiena_T0_3, Normal_T0_3, Normal_T0_2, Normal_T0_1, Normal_T0_1, Normal_T0_1], [Normal_T1_3, Normal_T1_2, QuasiPiena_T1_2], [QuasiPiena_T2_2, Normal_T2_2]]|\n",
      "|[[Normal_T0_3, Normal_T0_3, Normal_T0_1, Normal_T0_1, Normal_T0_2, Normal_T0_1, Normal_T0_0, Normal_T0_1, Normal_T0_1, Normal_T0_3, Normal_T0_1, Normal_T0_1, Normal_T0_2, QuasiPiena_T0_2, Normal_T0_1, Normal_T0_1, QuasiPiena_T0_2, Normal_T0_3, QuasiPiena_T0_1, Normal_T0_3, Normal_T0_2, Normal_T0_2, Normal_T0_1, Normal_T0_1, Normal_T0_3, Normal_T0_1, Normal_T0_2, Normal_T0_2, Normal_T0_2, QuasiPiena_T0_3, Normal_T0_3, Normal_T0_2, Normal_T0_2, Normal_T0_1, Normal_T0_1], [Normal_T1_3, Normal_T1_2, QuasiPiena_T1_2], [QuasiPiena_T2_2, Normal_T2_2]]|\n",
      "|[[Normal_T0_3, Normal_T0_3, Normal_T0_2, Normal_T0_2, Normal_T0_2, Normal_T0_2, Normal_T0_1, Normal_T0_0, Normal_T0_2, Normal_T0_2, Normal_T0_2, Normal_T0_2, QuasiPiena_T0_3, Normal_T0_1, Normal_T0_2, QuasiPiena_T0_2, Normal_T0_3, QuasiPiena_T0_1, Normal_T0_3, Normal_T0_2, Normal_T0_2, Normal_T0_1, Normal_T0_2, Normal_T0_3, Normal_T0_2, Normal_T0_3, Normal_T0_2, Normal_T0_1, QuasiPiena_T0_3, Normal_T0_3, Normal_T0_2, Normal_T0_3, Normal_T0_1, Normal_T0_2], [Normal_T1_3, Normal_T1_3, QuasiPiena_T1_3], [QuasiPiena_T2_3, Normal_T2_2]]             |\n",
      "|[[Normal_T0_3, Normal_T0_3, Normal_T0_1, Normal_T0_1, Normal_T0_2, Normal_T0_1, Normal_T0_1, Normal_T0_2, Normal_T0_0, Normal_T0_3, Normal_T0_1, Normal_T0_2, Normal_T0_2, QuasiPiena_T0_2, Normal_T0_1, Normal_T0_1, QuasiPiena_T0_3, QuasiPiena_T0_2, Normal_T0_2, Normal_T0_1, Normal_T0_2, Normal_T0_2, Normal_T0_1, Normal_T0_2, Normal_T0_1, Normal_T0_2, Normal_T0_2, Normal_T0_2, Normal_T0_3, Normal_T0_2, Normal_T0_1, Normal_T0_2, Normal_T0_1], [Normal_T1_2, QuasiPiena_T1_2], [QuasiPiena_T2_2, Normal_T2_3]]                                           |\n",
      "|[[Normal_T0_2, Normal_T0_2, Normal_T0_3, Normal_T0_3, Normal_T0_2, Normal_T0_3, Normal_T0_3, Normal_T0_3, Normal_T0_0, Normal_T0_3, Normal_T0_2, Normal_T0_2, QuasiPiena_T0_2, Normal_T0_3, Normal_T0_3, QuasiPiena_T0_2, Normal_T0_2, QuasiPiena_T0_3, Normal_T0_1, Normal_T0_2, Normal_T0_2, Normal_T0_3, Normal_T0_1, Normal_T0_2, Normal_T0_2, Normal_T0_2, QuasiPiena_T0_2, Normal_T0_2, Normal_T0_2, Normal_T0_3, Normal_T0_3, Normal_T0_3], [Normal_T1_2, Normal_T1_2, QuasiPiena_T1_2], [QuasiPiena_T2_2, Normal_T2_2]]                                       |\n",
      "|[[Normal_T0_3, Normal_T0_3, Normal_T0_1, Normal_T0_1, Normal_T0_1, Normal_T0_1, Normal_T0_1, Normal_T0_2, Normal_T0_1, Normal_T0_3, Normal_T0_0, Normal_T0_1, Normal_T0_1, QuasiPiena_T0_2, Normal_T0_1, Normal_T0_1, QuasiPiena_T0_2, Normal_T0_3, QuasiPiena_T0_1, Normal_T0_2, Normal_T0_1, Normal_T0_1, Normal_T0_1, Normal_T0_1, Normal_T0_2, Normal_T0_1, Normal_T0_2, Normal_T0_2, Normal_T0_2, QuasiPiena_T0_3, Normal_T0_3, Normal_T0_2, Normal_T0_2, Normal_T0_1, Normal_T0_1], [Normal_T1_3, Normal_T1_2, QuasiPiena_T1_2], [QuasiPiena_T2_2, Normal_T2_2]]|\n",
      "|[[Normal_T0_2, Normal_T0_2, Normal_T0_1, Normal_T0_2, Normal_T0_1, Normal_T0_1, Normal_T0_1, Normal_T0_2, Normal_T0_2, Normal_T0_2, Normal_T0_1, Normal_T0_0, Normal_T0_1, QuasiPiena_T0_2, Normal_T0_2, Normal_T0_2, QuasiPiena_T0_2, Normal_T0_3, QuasiPiena_T0_1, Normal_T0_2, Normal_T0_1, Normal_T0_1, Normal_T0_2, Normal_T0_1, Normal_T0_2, Normal_T0_1, Normal_T0_2, Normal_T0_1, Normal_T0_2, QuasiPiena_T0_3, Normal_T0_2, Normal_T0_1, Normal_T0_2, Normal_T0_1, Normal_T0_1], [Normal_T1_3, Normal_T1_2, QuasiPiena_T1_2], [QuasiPiena_T2_2, Normal_T2_2]]|\n",
      "|[[Normal_T0_2, Normal_T0_2, Normal_T0_2, Normal_T0_2, Normal_T0_1, Normal_T0_1, Normal_T0_2, Normal_T0_2, Normal_T0_2, Normal_T0_2, Normal_T0_1, Normal_T0_1, Normal_T0_0, QuasiPiena_T0_2, Normal_T0_2, Normal_T0_2, QuasiPiena_T0_1, Normal_T0_2, QuasiPiena_T0_2, Normal_T0_2, Normal_T0_1, Normal_T0_1, Normal_T0_2, Normal_T0_1, Normal_T0_2, Normal_T0_1, Normal_T0_2, Normal_T0_1, Normal_T0_3, QuasiPiena_T0_2, Normal_T0_2, Normal_T0_1, Normal_T0_2, Normal_T0_1, Normal_T0_1], [Normal_T1_2, Normal_T1_2, QuasiPiena_T1_2], [QuasiPiena_T2_2, Normal_T2_1]]|\n",
      "|[[Normal_T0_3, Normal_T0_3, Normal_T0_2, Normal_T0_2, Normal_T0_2, Normal_T0_2, Normal_T0_2, Normal_T0_3, Normal_T0_2, Normal_T0_2, Normal_T0_2, Normal_T0_2, Normal_T0_2, QuasiPiena_T0_0, Normal_T0_2, Normal_T0_1, QuasiPiena_T0_2, Normal_T0_3, QuasiPiena_T0_2, Normal_T0_1, Normal_T0_1, Normal_T0_1, Normal_T0_3, Normal_T0_1, Normal_T0_1, Normal_T0_1, Normal_T0_1, Normal_T0_2, Normal_T0_3, QuasiPiena_T0_3, Normal_T0_3, Normal_T0_2, Normal_T0_1, Normal_T0_2, Normal_T0_2], [Normal_T1_3, Normal_T1_0, QuasiPiena_T1_0], [QuasiPiena_T2_0, Normal_T2_2]]|\n",
      "|[[Normal_T0_3, Normal_T0_3, Normal_T0_1, Normal_T0_1, Normal_T0_2, Normal_T0_1, Normal_T0_1, Normal_T0_1, Normal_T0_1, Normal_T0_3, Normal_T0_1, Normal_T0_2, Normal_T0_2, QuasiPiena_T0_2, Normal_T0_0, Normal_T0_2, QuasiPiena_T0_3, QuasiPiena_T0_1, Normal_T0_3, Normal_T0_2, Normal_T0_2, Normal_T0_1, Normal_T0_1, Normal_T0_3, Normal_T0_2, Normal_T0_2, Normal_T0_2, Normal_T0_1, Normal_T0_3, Normal_T0_2, Normal_T0_2, Normal_T0_1, Normal_T0_1], [Normal_T1_2, QuasiPiena_T1_2], [QuasiPiena_T2_2, Normal_T2_3]]                                           |\n",
      "|[[Normal_T0_3, Normal_T0_3, Normal_T0_1, Normal_T0_1, Normal_T0_2, Normal_T0_1, Normal_T0_1, Normal_T0_2, Normal_T0_1, Normal_T0_3, Normal_T0_1, Normal_T0_2, Normal_T0_2, QuasiPiena_T0_1, Normal_T0_2, Normal_T0_0, QuasiPiena_T0_3, QuasiPiena_T0_2, Normal_T0_2, Normal_T0_1, Normal_T0_2, Normal_T0_2, Normal_T0_1, Normal_T0_2, Normal_T0_1, Normal_T0_2, Normal_T0_2, Normal_T0_2, Normal_T0_3, Normal_T0_2, Normal_T0_1, Normal_T0_2, Normal_T0_1], [Normal_T1_1, QuasiPiena_T1_1], [QuasiPiena_T2_1, Normal_T2_3]]                                           |\n",
      "|[[Normal_T0_1, Normal_T0_1, Normal_T0_3, Normal_T0_3, Normal_T0_1, Normal_T0_2, Normal_T0_2, Normal_T0_2, Normal_T0_3, Normal_T0_2, Normal_T0_2, Normal_T0_2, Normal_T0_1, QuasiPiena_T0_2, Normal_T0_3, Normal_T0_3, QuasiPiena_T0_0, Normal_T0_1, QuasiPiena_T0_2, Normal_T0_2, Normal_T0_2, Normal_T0_2, Normal_T0_3, Normal_T0_2, Normal_T0_2, Normal_T0_2, Normal_T0_2, Normal_T0_1, Normal_T0_3, QuasiPiena_T0_1, Normal_T0_1, Normal_T0_1, Normal_T0_3, Normal_T0_2, Normal_T0_2], [Normal_T1_1, Normal_T1_2, QuasiPiena_T1_2], [QuasiPiena_T2_2, Normal_T2_0]]|\n",
      "|[[Normal_T0_1, Normal_T0_1, Normal_T0_2, Normal_T0_3, Normal_T0_3, Normal_T0_3, Normal_T0_2, Normal_T0_3, Normal_T0_3, Normal_T0_2, QuasiPiena_T0_3, QuasiPiena_T0_1, Normal_T0_0, QuasiPiena_T0_3, Normal_T0_3, Normal_T0_3, Normal_T0_2, Normal_T0_3, Normal_T0_3, Normal_T0_3, Normal_T0_3, Normal_T0_2, QuasiPiena_T0_0, Normal_T0_1, Normal_T0_2, Normal_T0_3, Normal_T0_3], [Normal_T1_0, Normal_T1_3, QuasiPiena_T1_3], [QuasiPiena_T2_3, Normal_T2_1]]                                                                                                        |\n",
      "|[[Normal_T0_2, Normal_T0_3, Normal_T0_1, Normal_T0_1, Normal_T0_1, Normal_T0_1, Normal_T0_1, Normal_T0_1, Normal_T0_2, Normal_T0_3, Normal_T0_1, Normal_T0_1, Normal_T0_2, QuasiPiena_T0_2, Normal_T0_1, Normal_T0_2, QuasiPiena_T0_2, Normal_T0_3, QuasiPiena_T0_0, Normal_T0_3, Normal_T0_2, Normal_T0_2, Normal_T0_1, Normal_T0_2, Normal_T0_3, Normal_T0_2, Normal_T0_3, Normal_T0_2, Normal_T0_2, QuasiPiena_T0_3, Normal_T0_3, Normal_T0_2, Normal_T0_2, Normal_T0_1, Normal_T0_1], [Normal_T1_3, Normal_T1_2, QuasiPiena_T1_2], [QuasiPiena_T2_2, Normal_T2_2]]|\n",
      "|[[Normal_T0_3, Normal_T0_3, Normal_T0_2, Normal_T0_3, Normal_T0_2, Normal_T0_2, Normal_T0_3, Normal_T0_3, Normal_T0_2, Normal_T0_1, Normal_T0_2, Normal_T0_2, Normal_T0_2, QuasiPiena_T0_1, Normal_T0_3, Normal_T0_2, QuasiPiena_T0_2, Normal_T0_3, QuasiPiena_T0_3, Normal_T0_0, Normal_T0_1, Normal_T0_2, Normal_T0_3, Normal_T0_2, Normal_T0_1, Normal_T0_2, Normal_T0_1, Normal_T0_2, QuasiPiena_T0_3, Normal_T0_2, Normal_T0_2, Normal_T0_2, Normal_T0_3, Normal_T0_2], [Normal_T1_3, Normal_T1_1, QuasiPiena_T1_1], [QuasiPiena_T2_1, Normal_T2_2]]             |\n",
      "+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(len(df_list))\n",
    "df_list[0].show(20,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "plastic-montreal",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#prefixspan to obtain sequence and frequence\n",
    "prefix_list = []\n",
    "len_prefix_list = []\n",
    "for df in df_list:\n",
    "    print(support)\n",
    "    prefixSpan = PrefixSpan(minSupport=support, maxPatternLength=5,\n",
    "                        maxLocalProjDBSize=5000)\n",
    "    prefix=prefixSpan.findFrequentSequentialPatterns(df)   \n",
    "    prefix_list.append(prefix)\n",
    "    len_prefix=prefix.count()    \n",
    "    prefix.show(len_prefix,False)\n",
    "    len_prefix_list.append(len_prefix)\n",
    "\n",
    "\n",
    "# counter=[]\n",
    "# for support in supports:\n",
    "#     print(support)\n",
    "#     prefixSpan = PrefixSpan(minSupport=support, maxPatternLength=5,\n",
    "#                         maxLocalProjDBSize=5000)\n",
    "#     prefix=prefixSpan.findFrequentSequentialPatterns(df)   \n",
    "#     len_prefix=prefix.count()    \n",
    "# #     prefix.show(len_prefix,False)\n",
    "#     counter.append(len_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "recent-guarantee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "101579"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len_prefix_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "reliable-brooks",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_list = []\n",
    "\n",
    "for prefix in prefix_list:\n",
    "    pre=prefix.rdd.map(tuple)\n",
    "    pre_list.append(pre)\n",
    "\n",
    "# pre=prefix.rdd.map(tuple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "rising-seattle",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre.collect()\n",
    "\n",
    "# [([['QuasiPiena_T0_1']], 22981),\n",
    "#  ([['Piena_T1_1']], 9208),\n",
    "#  ([['QuasiPiena_T2_1']], 21641),\n",
    "#  ([['Piena_T2_2']], 12794),\n",
    "#  ([['QuasiPiena_T2_3']], 18047),\n",
    "#  ([['Piena_T2_3']], 6994),\n",
    "#  ([['Piena_T1_0']], 17930),\n",
    "#  ([['QuasiPiena_T0_3']], 18099), ... ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "marine-peripheral",
   "metadata": {},
   "outputs": [],
   "source": [
    "def giveSelected(line):\n",
    "    seq=line[0]\n",
    "    found=False   \n",
    "    for window in seq:\n",
    "        for el in window:\n",
    "            if 'T0' in el and '_0' in el:\n",
    "                found=True\n",
    "                break\n",
    "    return found "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "supported-anchor",
   "metadata": {},
   "outputs": [],
   "source": [
    "giveT0_list = []\n",
    "\n",
    "for pre in pre_list:\n",
    "    giveT0=pre.filter(giveSelected)\n",
    "    giveT0_list.append(giveT0)\n",
    "\n",
    "# giveT0=pre.filter(giveSelected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "remarkable-assumption",
   "metadata": {},
   "outputs": [],
   "source": [
    "# giveT0.collect()\n",
    "\n",
    "#CONFIGURAZIONE 30min, 1000m, 0 supp, (3,3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "stone-relief",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df2=giveT0.toDF().withColumnRenamed('_1','sequence')\n",
    "# df2=df2.withColumnRenamed('_2','freq')#.show(len_prefix,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "gross-happiness",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "meaning-suite",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapValues(line):\n",
    "    seq=line[0]\n",
    "    final=''\n",
    "    #voc[seq]=line[1]\n",
    "    for i,window in enumerate(seq):\n",
    "        if i>0:\n",
    "            final+='-'\n",
    "        for j,el in enumerate(window):\n",
    "            if j>0:\n",
    "                final+=','\n",
    "            final+=el\n",
    "    final+=(';'+str(line[1])+';'+str(i))\n",
    "    return final  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "coordinated-stroke",
   "metadata": {},
   "outputs": [],
   "source": [
    "mapDict_list = []\n",
    "\n",
    "for giveT0 in giveT0_list:\n",
    "    mapDict=giveT0.map(mapValues)\n",
    "    mapDict_list.append(mapDict)\n",
    "\n",
    "\n",
    "# mapDict=giveT0.map(mapValues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "critical-idaho",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mapDict.collect()\n",
    "\n",
    "# ['QuasiVuota_T0_0;282;0',\n",
    "#  'Vuota_T0_0;192;0',\n",
    "#  'QuasiVuota_T0_0,Vuota_T0_3;2;0',\n",
    "#  'QuasiVuota_T0_0,Vuota_T0_3-QuasiVuota_T1_1;1;1',\n",
    "#  'QuasiVuota_T0_0,Vuota_T0_3-QuasiVuota_T1_1,QuasiVuota_T1_3;1;1', ...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "assigned-employment",
   "metadata": {},
   "outputs": [],
   "source": [
    "li_list =[]\n",
    "\n",
    "for mapDict in mapDict_list:\n",
    "    li=mapDict.collect()\n",
    "    li_list.append(li)\n",
    "\n",
    "# li=mapDict.collect()\n",
    "# voc={}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "suffering-military",
   "metadata": {},
   "outputs": [],
   "source": [
    "voc_list = []\n",
    "for li in li_list:\n",
    "    voc={}\n",
    "    for el in li:\n",
    "        splits=el.split(';')\n",
    "        voc[splits[0]]=int(splits[1])\n",
    "    voc_list.append(voc)\n",
    "\n",
    "\n",
    "# for el in li:\n",
    "#     splits=el.split(';')\n",
    "#     voc[splits[0]]=int(splits[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "fluid-story",
   "metadata": {},
   "outputs": [],
   "source": [
    "# voc\n",
    "\n",
    "# {'QuasiVuota_T0_0': 88948,\n",
    "#  'Vuota_T0_0': 44987,\n",
    "#  'Vuota_T0_0-QuasiVuota_T1_2': 33837,\n",
    "#  'QuasiVuota_T0_0-Vuota_T1_0': 26011,\n",
    "#  'QuasiVuota_T0_2,Vuota_T0_0': 34141, ...}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "answering-million",
   "metadata": {},
   "outputs": [],
   "source": [
    "repeated_list = []\n",
    "\n",
    "for voc in voc_list:\n",
    "    repeated_el_window=0\n",
    "    for el in voc.keys():\n",
    "        flag_rep=False\n",
    "        windows=el.split('-')\n",
    "        for w in windows:\n",
    "            tot_items=len(w.split(','))\n",
    "            set_items=len(set(w.split(',')))\n",
    "            if tot_items!=set_items:\n",
    "                repeated_el_window+=1\n",
    "                break\n",
    "    repeated_list.append(repeated_el_window)\n",
    "    \n",
    "# repeated_list[0]\n",
    "\n",
    "\n",
    "# repeated_el_window=0\n",
    "# for el in voc.keys():\n",
    "#     flag_rep=False\n",
    "#     windows=el.split('-')\n",
    "#     for w in windows:\n",
    "#         tot_items=len(w.split(','))\n",
    "#         set_items=len(set(w.split(',')))\n",
    "#         if tot_items!=set_items:\n",
    "#             repeated_el_window+=1\n",
    "#             break\n",
    "# repeated_el_window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "infrared-world",
   "metadata": {},
   "outputs": [],
   "source": [
    "voc_supports_list = []\n",
    "keys_list = []\n",
    "values_list = []\n",
    "\n",
    "for voc in voc_list:\n",
    "    voc_supports={}\n",
    "    for el in voc.keys():    \n",
    "        if len(el.split('-'))>1:        \n",
    "            num=int(voc[el])       \n",
    "            string=''\n",
    "            tot=el.split('-')[:-1]\n",
    "            for k,station in enumerate(tot):\n",
    "                if k>0:\n",
    "                    string+='-'\n",
    "                string+=station\n",
    "            #print(string)\n",
    "            den=int(voc[string])\n",
    "            voc_supports[el]=str(num/den)+' - '+str(voc[el])\n",
    "    voc_supports_list.append(voc_supports)\n",
    "#     keys=list(voc_supports.keys())\n",
    "#     keys_list.append(keys)\n",
    "#     values=list(voc_supports.values())\n",
    "#     values_list.append(values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "medieval-hartford",
   "metadata": {},
   "outputs": [],
   "source": [
    "# voc_supports\n",
    "\n",
    "# {'Vuota_T0_0-QuasiVuota_T1_2': '0.7521506212905951 - 33837',\n",
    "#  'QuasiVuota_T0_0-Vuota_T1_0': '0.29242928452579037 - 26011',\n",
    "#  'Vuota_T0_0-QuasiVuota_T2_3': '0.4979438504456843 - 22401',\n",
    "#  'Vuota_T0_0-Vuota_T2_3': '0.28939471402849715 - 13019',\n",
    "#  'QuasiVuota_T0_0-QuasiVuota_T2_0': '0.3469780096236003 - 30863', ...}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "informative-blogger",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sort vocabulary by decreasing values and sort within each window\n",
    "# tot_frequence=0\n",
    "# for key in voc_supports:    \n",
    "#     splitted=key.split('-')      \n",
    "#     splitted.sort()\n",
    "#     tmp_frequence=int(voc_supports[key].split(' - ')[1])\n",
    "#     tot_frequence+=tmp_frequence\n",
    "# for key in voc_supports:\n",
    "#     freq= int(voc_supports[key].split(' - ')[1])\n",
    "#     #print(freq)\n",
    "# #     perc_value=float(\"{:.2f}\".format(freq*100/tot_frequence))\n",
    "#     voc_supports[key]=str(voc_supports[key].split(' - ')[0])+' - ' + str(freq)#+' - '+str(perc_value)+'%'  \n",
    "# # voc_supports = dict(sorted(voc_supports.items(), key=operator.itemgetter(1),reverse=True))\n",
    "# voc_supports = dict(sorted(voc_supports.items(), \n",
    "#                            key=lambda v: (float(v[1].split(' - ')[0]), int(v[1].split(' - ')[1])),\n",
    "#                            reverse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "combined-parallel",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, voc_supports in enumerate(voc_supports_list):\n",
    "    \n",
    "    #sort vocabulary by decreasing values of confidence\n",
    "    voc_supports = dict(sorted(voc_supports.items(), \n",
    "                               key=lambda v: (float(v[1].split(' - ')[0]), int(v[1].split(' - ')[1])),\n",
    "                               reverse=True))\n",
    "    \n",
    "    #save file ordered by confidence\n",
    "    file = open(eval(\"output_file_conf\"+str(i+1)), \"w\")\n",
    "    list_pattern=[]\n",
    "    for el in voc_supports:\n",
    "        key_list=[]\n",
    "        for e in el.split('-'):\n",
    "            key_list.append([e])\n",
    "        list_pattern.append([[key_list], [voc_supports[el]]])\n",
    "    file.write('TIME SLOT: ' + eval(\"output_file_conf\"+str(i+1)).split('_')[1] + ' Pattern, Confidence-Frequence'+'\\n')\n",
    "    file.write(f'Total number of input patterns: {len(voc_supports)}'+'\\n')\n",
    "    for el in list_pattern:  \n",
    "        file.write(str(el)+ '\\n')    \n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "advised-literacy",
   "metadata": {},
   "outputs": [],
   "source": [
    "end = time.time()\n",
    "print(f'The time of execution is: {end-start} s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pretty-birth",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark (Yarn)",
   "language": "python",
   "name": "pyspark_yarn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
