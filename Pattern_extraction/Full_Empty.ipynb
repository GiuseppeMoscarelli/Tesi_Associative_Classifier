{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "meaningful-loading",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "hidden-harvard",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Bad key \"text.kerning_factor\" on line 4 in\n",
      "/opt/anaconda3/envs/bigdatalab_cpu_202101/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test_patch.mplstyle.\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "https://github.com/matplotlib/matplotlib/blob/v3.1.2/matplotlibrc.template\n",
      "or from the matplotlib source distribution\n"
     ]
    }
   ],
   "source": [
    "#IMPORTS\n",
    "from datetime import datetime\n",
    "from pyspark.ml.fpm import PrefixSpan\n",
    "from pyspark.sql import Row\n",
    "import pyspark.sql.functions as F\n",
    "from math import sin, cos, sqrt, atan2, radians \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import operator\n",
    "\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "environmental-finding",
   "metadata": {},
   "outputs": [],
   "source": [
    "#parameters\n",
    "support=0\n",
    "support_str = \"0\" #used for paths\n",
    "interval=15 #time window\n",
    "maxDelta=3 #how many spatial delta\n",
    "th=1 #distance\n",
    "window_size=4 #how many time delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "geographic-adams",
   "metadata": {},
   "outputs": [],
   "source": [
    "#FILES\n",
    "inputPath  = \"file:///home/bigdata-01QYD/s270240/bike_sharing/filtered_status.csv\"\n",
    "STATION_PATH=\"file:///home/bigdata-01QYD/s270240/bike_sharing/station.csv\"\n",
    "#output files\n",
    "folder_path = f'./Results_extraction/Full_Empty/Full_Empty_{interval}_{int(th*1000)}_{support_str}({window_size}-{maxDelta})'\n",
    "output_file=f'{folder_path}/results_{int(th*1000)}_{support_str}_ordered_by_confidence.txt'\n",
    "output_file2=f'{folder_path}/results_{int(th*1000)}_{support_str}_Vuota_Piena_ordered_by_confidence.txt'\n",
    "output_file3=f'{folder_path}/results_{int(th*1000)}_{support_str}_diff_delta_ordered_by_confidence.txt'\n",
    "output_file4=f'{folder_path}/results_{int(th*1000)}_{support_str}_ordered_by_support.txt'\n",
    "output_file5=f'{folder_path}/results_{int(th*1000)}_{support_str}_Vuota_Piena_ordered_by_support.txt'\n",
    "output_file6=f'{folder_path}/results_{int(th*1000)}_{support_str}_diff_delta_ordered_by_support.txt'\n",
    "img_support=f'{window_size}-{maxDelta}-{int(th*1000)}-{support_str}.jpg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "seasonal-minimum",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputDF = spark.read.format(\"csv\")\\\n",
    ".option(\"delimiter\", \",\")\\\n",
    ".option(\"header\", True)\\\n",
    ".option(\"inferSchema\", True).load(inputPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "separate-driver",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputDF=inputDF.filter(\"bikes_available is not null\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "individual-westminster",
   "metadata": {},
   "outputs": [],
   "source": [
    "#filter for these fields\n",
    "filteredDF = inputDF.filter(\"docks_available<>0 OR bikes_available<>0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "expensive-season",
   "metadata": {},
   "outputs": [],
   "source": [
    "#determine if the station is full, empty or partially full\n",
    "def stateFunction(docks_available, bikes_available):\n",
    "    if docks_available==0:\n",
    "        return 1\n",
    "    elif bikes_available==0:\n",
    "        return 0\n",
    "    else:\n",
    "        return 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "complete-profit",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.stateFunction(docks_available, bikes_available)>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.udf.register(\"state\", stateFunction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "associate-frost",
   "metadata": {},
   "outputs": [],
   "source": [
    "getStatusDF = filteredDF.selectExpr(\"station_id\",\"time\", \"state(docks_available,bikes_available) as status\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "characteristic-morris",
   "metadata": {},
   "outputs": [],
   "source": [
    "# getStatusDF.show(100,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "serious-contrary",
   "metadata": {},
   "outputs": [],
   "source": [
    "#filter only full or empty stations\n",
    "full_empty=getStatusDF.filter(\"status==1  or status==0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "complicated-mobility",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_empty_count = full_empty.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "modified-daniel",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.1861458605841708"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_empty_count/getStatusDF.count()*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cheap-surgeon",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+-------------------+\n",
      "|summary|        station_id|             status|\n",
      "+-------+------------------+-------------------+\n",
      "|  count|            853763|             853763|\n",
      "|   mean| 54.90071952052267|0.38197719976152633|\n",
      "| stddev|19.220736591922407|0.48587127423858284|\n",
      "|    min|                 2|                  0|\n",
      "|    max|                84|                  1|\n",
      "+-------+------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "full_empty.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "unknown-initial",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a view\n",
    "full_empty.createOrReplaceTempView(\"readings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "amended-executive",
   "metadata": {},
   "outputs": [],
   "source": [
    "#select station, year, month, day, hour, minute, status ordered by time\n",
    "ss=spark.sql(\"\"\"SELECT  station_id , YEAR(time) as year, MONTH(time) as month, DAY(time) as day, HOUR(time)as hour, MINUTE(time) as minute, status\n",
    "FROM readings\n",
    "GROUP BY station_id, year, month, day,hour,minute, status\n",
    "ORDER BY  station_id,year, month,day, hour,minute\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "documented-iceland",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create rdd and group into interval\n",
    "my_rdd=ss.rdd.map(tuple)\n",
    "rdd=my_rdd.map(lambda line: (line[0],line[1],line[2], line[3], line[4], int(line[5]/interval), line[6])).distinct()\n",
    "# rdd.collect()\n",
    "\n",
    "# [(22, 2015, 6, 19, 7, 0, '0'),\n",
    "#  (22, 2015, 6, 19, 12, 1, '0'),\n",
    "#  (41, 2014, 4, 18, 13, 1, '0'),\n",
    "#  (47, 2013, 11, 7, 6, 1, '0'),\n",
    "#  (50, 2014, 11, 4, 8, 0, '1'),\n",
    "#  (50, 2015, 1, 13, 6, 0, '1'),\n",
    "#  (50, 2015, 1, 20, 18, 1, '1'),\n",
    "#  (56, 2015, 1, 31, 21, 1, '1'),\n",
    "#  (56, 2015, 5, 21, 16, 0, '0'), ...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "common-tracker",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get distinct stations to calculate distances\n",
    "id_stations=rdd.map(lambda line: line[0]).distinct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "developing-lemon",
   "metadata": {},
   "outputs": [],
   "source": [
    "tot_id_stations=id_stations.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "hindu-irrigation",
   "metadata": {},
   "outputs": [],
   "source": [
    "#list of all stations\n",
    "# tot_id_stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "quick-storage",
   "metadata": {},
   "outputs": [],
   "source": [
    "#obtain timestamp and info\n",
    "def getMap(line):\n",
    "    id_station=str(line[0])\n",
    "    year=int(line[1])\n",
    "    month=int(line[2])\n",
    "    day=int(line[3])\n",
    "    hour=int(line[4])\n",
    "    minute=int(line[5])   \n",
    "    timestamp= datetime(year,month, day, hour, minute)  \n",
    "    status=int(line[6])\n",
    "    if status==0:\n",
    "        status='Vuota'\n",
    "    else:\n",
    "        status='Piena'\n",
    "    info=id_station.split('.')[0]+'_'+status\n",
    "    return ( (timestamp,info))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "saved-stopping",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_map=rdd.map(getMap)\n",
    "# get_map.collect()\n",
    "\n",
    "# [(datetime.datetime(2015, 6, 19, 7, 0), '22_Vuota'),\n",
    "#  (datetime.datetime(2015, 6, 19, 12, 1), '22_Vuota'),\n",
    "#  (datetime.datetime(2014, 4, 18, 13, 1), '41_Vuota'),\n",
    "#  (datetime.datetime(2013, 11, 7, 6, 1), '47_Vuota'), ...]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "representative-scholar",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for each timestamp obtain info\n",
    "reduceK=get_map.reduceByKey(lambda l1,l2 :(l1+','+l2)).sortByKey()\n",
    "# reduceK.collect()\n",
    "\n",
    "# [(datetime.datetime(2013, 8, 29, 10, 0), '67_Vuota,70_Vuota,7_Vuota,4_Vuota,13_Vuota'),\n",
    "#  (datetime.datetime(2013, 8, 29, 10, 1), '7_Vuota,4_Vuota,67_Vuota,13_Vuota,45_Piena'),\n",
    "#  (datetime.datetime(2013, 8, 29, 11, 0), '4_Vuota,45_Piena,67_Vuota,64_Piena'),\n",
    "#  (datetime.datetime(2013, 8, 29, 12, 0), '69_Vuota'),\n",
    "#  (datetime.datetime(2013, 8, 29, 12, 1), '64_Piena,69_Vuota'),\n",
    "#  (datetime.datetime(2013, 8, 29, 13, 0), '69_Vuota,76_Vuota,64_Piena'), ...]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "experienced-library",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_df=reduceK.toDF()\n",
    "# my_df.collect()\n",
    "\n",
    "# [Row(_1=datetime.datetime(2013, 8, 29, 8, 0), _2='67_Vuota,70_Vuota,7_Vuota,4_Vuota,13_Vuota'),\n",
    "#  Row(_1=datetime.datetime(2013, 8, 29, 8, 1), _2='7_Vuota,4_Vuota,67_Vuota,13_Vuota,45_Piena'),\n",
    "#  Row(_1=datetime.datetime(2013, 8, 29, 9, 0), _2='4_Vuota,45_Piena,67_Vuota,64_Piena'),\n",
    "#  Row(_1=datetime.datetime(2013, 8, 29, 10, 0), _2='69_Vuota'),\n",
    "#  Row(_1=datetime.datetime(2013, 8, 29, 10, 1), _2='64_Piena,69_Vuota'), ...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "musical-frederick",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_df.createOrReplaceTempView(\"view\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "suburban-interview",
   "metadata": {},
   "outputs": [],
   "source": [
    "s2=spark.sql(\"\"\"SELECT ROW_NUMBER() OVER(ORDER BY _1,_2) as id ,_1, _2\n",
    "FROM view \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "future-illness",
   "metadata": {},
   "outputs": [],
   "source": [
    "#identifier of the timestamp, info\n",
    "rdd_scheme=s2.rdd.map(tuple).map(lambda line: (line[0], line[2]))\n",
    "# rdd_scheme.collect()\n",
    "\n",
    "# [(1, '67_Vuota,70_Vuota,7_Vuota,4_Vuota,13_Vuota'),\n",
    "#  (2, '7_Vuota,4_Vuota,67_Vuota,13_Vuota,45_Piena'),\n",
    "#  (3, '4_Vuota,45_Piena,67_Vuota,64_Piena'),\n",
    "#  (4, '69_Vuota'),\n",
    "#  (5, '64_Piena,69_Vuota'),\n",
    "#  (6, '69_Vuota,76_Vuota,64_Piena'), ... ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "pregnant-cradle",
   "metadata": {},
   "outputs": [],
   "source": [
    "#obtain window, station-status\n",
    "def giveSplit(line):   \n",
    "    id_window=( int(line[0] ))\n",
    "    lista=[]    \n",
    "    counter=id_window    \n",
    "    while counter>=1:\n",
    "        lista.append(('Window '+str(counter),(line[1])))\n",
    "        counter=counter-1\n",
    "        if (id_window-counter)==window_size:\n",
    "            return lista  \n",
    "    return lista"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "assured-custom",
   "metadata": {},
   "outputs": [],
   "source": [
    "mapData=rdd_scheme.flatMap(giveSplit)\n",
    "# mapData.collect()\n",
    "\n",
    "# [('Window 1', '67_Vuota,70_Vuota,7_Vuota,4_Vuota,13_Vuota'),\n",
    "#  ('Window 2', '7_Vuota,4_Vuota,67_Vuota,13_Vuota,45_Piena'),\n",
    "#  ('Window 1', '7_Vuota,4_Vuota,67_Vuota,13_Vuota,45_Piena'),\n",
    "#  ('Window 3', '4_Vuota,45_Piena,67_Vuota,64_Piena'),\n",
    "#  ('Window 2', '4_Vuota,45_Piena,67_Vuota,64_Piena'),\n",
    "#  ('Window 1', '4_Vuota,45_Piena,67_Vuota,64_Piena'),... ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "imposed-spell",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each window get all info\n",
    "all_keys=mapData.reduceByKey(lambda l1,l2:(l1+'-'+l2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "biological-conflict",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_keys.collect()\n",
    "\n",
    "# [('Window 1', '67_Vuota,70_Vuota,7_Vuota,4_Vuota,13_Vuota-7_Vuota,4_Vuota,67_Vuota,13_Vuota,45_Piena-4_Vuota,45_Piena,67_Vuota,64_Piena'),\n",
    "#  ('Window 2', '7_Vuota,4_Vuota,67_Vuota,13_Vuota,45_Piena-4_Vuota,45_Piena,67_Vuota,64_Piena-69_Vuota'),\n",
    "#  ('Window 3', '4_Vuota,45_Piena,67_Vuota,64_Piena-69_Vuota-64_Piena,69_Vuota'),\n",
    "#  ('Window 4', '69_Vuota-64_Piena,69_Vuota-69_Vuota,76_Vuota,64_Piena'),\n",
    "#  ('Window 5', '64_Piena,69_Vuota-69_Vuota,76_Vuota,64_Piena-69_Vuota,76_Vuota'), ...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "sharp-freight",
   "metadata": {},
   "outputs": [],
   "source": [
    "#finestra temporale\n",
    "def reduceKeys(line):   \n",
    "    lista=[]\n",
    "    #lista.append(line[0])\n",
    "    line_split=line[1].split(\"-\")\n",
    "    #return line_split[0]\n",
    "    count=len(line_split)\n",
    "    tot=[]\n",
    "    for val in range(count):\n",
    "        li=[]\n",
    "        stations=line_split[val].split(',')\n",
    "        for st in stations:\n",
    "            all_string_st=st.split('_')[0]+'_'+'T'+str(val)+'_'+st.split('_')[1]\n",
    "            li.append(all_string_st)\n",
    "        tot.append(li)\n",
    "    lista.append((line[0],(tot))) \n",
    "    return lista"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "thorough-carolina",
   "metadata": {},
   "outputs": [],
   "source": [
    "windows=all_keys.flatMap(reduceKeys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "french-croatia",
   "metadata": {},
   "outputs": [],
   "source": [
    "# windows.collect()\n",
    "\n",
    "# [('Window 1',\n",
    "#   [['67_T0_Vuota', '70_T0_Vuota', '7_T0_Vuota', '4_T0_Vuota', '13_T0_Vuota'],\n",
    "#    ['7_T1_Vuota', '4_T1_Vuota', '67_T1_Vuota', '13_T1_Vuota', '45_T1_Piena'],\n",
    "#    ['4_T2_Vuota', '45_T2_Piena', '67_T2_Vuota', '64_T2_Piena']]),\n",
    "#  ('Window 2',\n",
    "#   [['7_T0_Vuota', '4_T0_Vuota', '67_T0_Vuota', '13_T0_Vuota', '45_T0_Piena'],\n",
    "#    ['4_T1_Vuota', '45_T1_Piena', '67_T1_Vuota', '64_T1_Piena'],\n",
    "#    ['69_T2_Vuota']]), ... ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "greek-kidney",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save station file\n",
    "stationsDF = spark.read.format(\"csv\")\\\n",
    ".option(\"delimiter\", \",\")\\\n",
    ".option(\"header\", True)\\\n",
    ".option(\"inferSchema\", True).load(STATION_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "received-windows",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get only rows interested: only the used stations \n",
    "necessary_rows=stationsDF.filter(F.col(\"id\").isin(tot_id_stations)).sort(\"id\").rdd.map(tuple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "immune-broadcasting",
   "metadata": {},
   "outputs": [],
   "source": [
    "# necessary_rows.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "governmental-reader",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get info of stations about coordinates and name\n",
    "coordinates=necessary_rows.map(lambda line: (line[0],(str(line[2])+','+str(line[3]))))\n",
    "names_stations=necessary_rows.map(lambda line: (line[0],line[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "interested-brain",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_coo=coordinates.collect()\n",
    "# list_coo\n",
    "\n",
    "# [(2, '37.329732,-121.90178200000001'),\n",
    "#  (3, '37.330698,-121.888979'),\n",
    "#  (4, '37.333988,-121.894902'),... ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "tired-priest",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a dictionary in which the key is the station and value is the info about coordinates\n",
    "dic_co=coordinates.collectAsMap()\n",
    "dic_coordinates=sc.broadcast(dic_co)\n",
    "# dic_coordinates.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "satisfactory-syracuse",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to retrieve distance between 2 stations\n",
    "def getDistance(station1,station2):\n",
    "    # approximate radius of earth in km\n",
    "    R = 6373.0    \n",
    "    lat_a=float(station1.split(',')[0])\n",
    "    lat_b=float(station2.split(',')[0])\n",
    "    long_a=float(station1.split(',')[1])\n",
    "    long_b=float(station2.split(',')[1])\n",
    "    \n",
    "    lat1=radians(lat_a)\n",
    "    lat2=radians(lat_b)\n",
    "    lon1=radians(long_a)\n",
    "    lon2=radians(long_b)\n",
    "    \n",
    "    dlon = lon2 - lon1\n",
    "    dlat = lat2 - lat1\n",
    "\n",
    "    a = sin(dlat / 2)**2 + cos(lat1) * cos(lat2) * sin(dlon / 2)**2\n",
    "    c = 2 * atan2(sqrt(a), sqrt(1 - a))\n",
    "\n",
    "    distance = R * c\n",
    "    return distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "pending-washington",
   "metadata": {},
   "outputs": [],
   "source": [
    "#voc in which the key is a pair of stations and value is the distance\n",
    "voc_distances={}\n",
    "for i in range(len(list_coo)):\n",
    "    for j in range(i+1,len(list_coo)):\n",
    "        station1=list_coo[i][0]\n",
    "        station2=list_coo[j][0]\n",
    "        d_i=list_coo[i][1]\n",
    "        d_j=list_coo[j][1]\n",
    "        distance=getDistance(d_i,d_j)\n",
    "        id_stations=str(station1)+' '+str(station2)\n",
    "        voc_distances[id_stations]=distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "premier-enforcement",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.1374454650255135"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voc_distances['2 3']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "derived-hamburg",
   "metadata": {},
   "outputs": [],
   "source": [
    "# voc_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "streaming-netscape",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Applicazione “Delta Spaziale”\n",
    "def giveSpatialWindow(line):\n",
    "    lista=[]    \n",
    "    time0=line[1][0]    \n",
    "    dic={}\n",
    "    \n",
    "    count_windows=len(line[1])#tot windows\n",
    "\n",
    "    for station in time0:# only first window\n",
    "        act_station=int(station.split('_')[0])\n",
    "        #lista_station=[] \n",
    "        list_tmp=[]\n",
    "        \n",
    "        #for each window\n",
    "        for i,window in enumerate(line[1]):           \n",
    "            second_lista=[]\n",
    "            #for each element of a window\n",
    "            for all_el in window :\n",
    "                #second_lista=[]\n",
    "                act_all_el=int(all_el.split('_')[0])\n",
    "                state=all_el.split('_')[2]\n",
    "               \n",
    "                if act_station!=act_all_el:\n",
    "                    \n",
    "                    key=''\n",
    "                    if act_station<act_all_el:\n",
    "                        key=str(act_station)+' '+str(act_all_el)\n",
    "                    else:\n",
    "                        key=str(act_all_el)+' '+str(act_station)                    \n",
    "                    \n",
    "                    dist=voc_distances[key]\n",
    "                    if dist<=maxDelta*th:\n",
    "                        delta=0\n",
    "                        for d in range(1,maxDelta+1):\n",
    "                            if d*th>=dist:\n",
    "                                delta=d\n",
    "                                break                        \n",
    "                        string=state+'_'+'T'+str(i)+'_'+str(delta)\n",
    "                        second_lista.append(string)\n",
    "                else:\n",
    "                    string=state+'_'+'T'+str(i)+'_'+str(0)\n",
    "                    second_lista.append(string)\n",
    "                    \n",
    "            if len(second_lista)>0:\n",
    "                list_tmp.append(second_lista)\n",
    "        lista.append(((line[0]+'|'+str(act_station)),list_tmp))\n",
    "    \n",
    "    return lista"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "matched-mongolia",
   "metadata": {},
   "outputs": [],
   "source": [
    "spatial_app=windows.flatMap(giveSpatialWindow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "public-updating",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spatial_app.collect()\n",
    "\n",
    "# [('Window 1|67',\n",
    "#   [['Vuota_T0_0', 'Vuota_T0_2'],\n",
    "#    ['Vuota_T1_0', 'Piena_T1_3'],\n",
    "#    ['Piena_T2_3', 'Vuota_T2_0', 'Piena_T2_3']]),\n",
    "#  ('Window 1|70',\n",
    "#   [['Vuota_T0_2', 'Vuota_T0_0'],\n",
    "#    ['Vuota_T1_2', 'Piena_T1_3'],\n",
    "#    ['Piena_T2_3', 'Vuota_T2_2', 'Piena_T2_1']]),\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "elementary-conducting",
   "metadata": {},
   "outputs": [],
   "source": [
    "def row_seq(line):\n",
    "    true=line[1]\n",
    "    string=Row(sequence=true)\n",
    "    return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "active-fraction",
   "metadata": {},
   "outputs": [],
   "source": [
    "spatial=spatial_app.map(row_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "black-agenda",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a dataframe\n",
    "df=spatial.toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "proprietary-blues",
   "metadata": {},
   "outputs": [],
   "source": [
    "supports=[support]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "bottom-wonder",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|sequence                                                                                                                                                  |\n",
      "+----------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|[[Vuota_T0_0, Vuota_T0_1, Vuota_T0_1], [Vuota_T1_0, Vuota_T1_1, Vuota_T1_1], [Vuota_T2_1, Vuota_T2_0, Vuota_T2_1], [Vuota_T3_0, Vuota_T3_1], [Vuota_T4_0]]|\n",
      "|[[Vuota_T0_1, Vuota_T0_0, Vuota_T0_1], [Vuota_T1_1, Vuota_T1_1, Vuota_T1_0], [Vuota_T2_1, Vuota_T2_1, Vuota_T2_0], [Vuota_T3_1, Vuota_T3_1], [Vuota_T4_1]]|\n",
      "|[[Vuota_T0_1, Vuota_T0_1, Vuota_T0_0], [Vuota_T1_1, Vuota_T1_0, Vuota_T1_1], [Vuota_T2_0, Vuota_T2_1, Vuota_T2_1], [Vuota_T3_1, Vuota_T3_0], [Vuota_T4_1]]|\n",
      "|[[Vuota_T0_0], [Vuota_T1_0, Vuota_T1_2], [Vuota_T2_2, Piena_T2_3], [Piena_T3_3], [Piena_T4_3, Piena_T4_1]]                                                |\n",
      "|[[Vuota_T0_0, Vuota_T0_2], [Vuota_T1_2, Piena_T1_3], [Piena_T2_3], [Piena_T3_3, Piena_T3_1], [Vuota_T4_2, Piena_T4_3]]                                    |\n",
      "|[[Vuota_T0_0, Vuota_T0_1, Vuota_T0_1], [Vuota_T1_1, Vuota_T1_0, Vuota_T1_1], [Vuota_T2_0, Vuota_T2_1], [Vuota_T3_0]]                                      |\n",
      "|[[Vuota_T0_2, Vuota_T0_0], [Vuota_T1_0, Piena_T1_3], [Piena_T2_3], [Piena_T3_3, Piena_T3_3], [Vuota_T4_0, Piena_T4_3]]                                    |\n",
      "|[[Vuota_T0_1, Vuota_T0_0, Vuota_T0_1], [Vuota_T1_0, Vuota_T1_1, Vuota_T1_1], [Vuota_T2_1, Vuota_T2_0], [Vuota_T3_1]]                                      |\n",
      "|[[Vuota_T0_1, Vuota_T0_1, Vuota_T0_0], [Vuota_T1_1, Vuota_T1_1, Vuota_T1_0], [Vuota_T2_1, Vuota_T2_1], [Vuota_T3_1]]                                      |\n",
      "|[[Vuota_T0_0, Vuota_T0_1, Vuota_T0_1], [Vuota_T1_1, Vuota_T1_0], [Vuota_T2_1]]                                                                            |\n",
      "|[[Vuota_T0_0, Piena_T0_3], [Piena_T1_3], [Piena_T2_3, Piena_T2_3], [Vuota_T3_0, Piena_T3_3], [Vuota_T4_2]]                                                |\n",
      "|[[Vuota_T0_3, Piena_T0_0], [Piena_T1_0], [Piena_T2_0, Piena_T2_2], [Vuota_T3_3, Piena_T3_0], [Vuota_T4_3]]                                                |\n",
      "|[[Vuota_T0_1, Vuota_T0_0, Vuota_T0_1], [Vuota_T1_0, Vuota_T1_1], [Vuota_T2_0]]                                                                            |\n",
      "|[[Vuota_T0_1, Vuota_T0_1, Vuota_T0_0], [Vuota_T1_1, Vuota_T1_1], [Vuota_T2_1]]                                                                            |\n",
      "|[[Vuota_T0_0, Vuota_T0_1], [Vuota_T1_0]]                                                                                                                  |\n",
      "|[[Piena_T0_0], [Piena_T1_0, Piena_T1_2], [Vuota_T2_3, Piena_T2_0], [Vuota_T3_3], [Piena_T4_2, Vuota_T4_3]]                                                |\n",
      "|[[Vuota_T0_1, Vuota_T0_0], [Vuota_T1_1]]                                                                                                                  |\n",
      "|[[Piena_T0_0, Piena_T0_2], [Vuota_T1_3, Piena_T1_0], [Vuota_T2_3], [Piena_T3_2, Vuota_T3_3], [Piena_T4_2, Vuota_T4_3]]                                    |\n",
      "|[[Vuota_T0_0]]                                                                                                                                            |\n",
      "|[[Piena_T0_2, Piena_T0_0], [Vuota_T1_3, Piena_T1_2], [Vuota_T2_1], [Piena_T3_0, Vuota_T3_1], [Piena_T4_0, Vuota_T4_1]]                                    |\n",
      "+----------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(20,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "particular-disability",
   "metadata": {},
   "outputs": [],
   "source": [
    "#prefixspan to obtain sequence and frequence\n",
    "counter=[]\n",
    "for support in supports:\n",
    "    print(support)\n",
    "    prefixSpan = PrefixSpan(minSupport=support, maxPatternLength=5,\n",
    "                        maxLocalProjDBSize=5000)\n",
    "    prefix=prefixSpan.findFrequentSequentialPatterns(df)   \n",
    "    len_prefix=prefix.count()    \n",
    "    prefix.show(len_prefix,False)\n",
    "    counter.append(len_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "approximate-combine",
   "metadata": {},
   "outputs": [],
   "source": [
    "len_prefix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "announced-consistency",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre=prefix.rdd.map(tuple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "typical-jacob",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre.collect()\n",
    "\n",
    "# [([['Vuota_T1_1']], 18487),\n",
    "#  ([['Piena_T2_1']], 10252),\n",
    "#  ([['Piena_T1_2']], 14445), ...]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "civil-house",
   "metadata": {},
   "outputs": [],
   "source": [
    "def giveSelected(line):\n",
    "    seq=line[0]\n",
    "    found=False   \n",
    "    for window in seq:\n",
    "        for el in window:\n",
    "            if 'T0' in el and '_0' in el:\n",
    "                found=True\n",
    "                break\n",
    "    return found "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "funky-arabic",
   "metadata": {},
   "outputs": [],
   "source": [
    "giveT0=pre.filter(giveSelected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rural-teddy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# giveT0.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "front-relaxation",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2=giveT0.toDF().withColumnRenamed('_1','sequence')\n",
    "df2=df2.withColumnRenamed('_2','freq')#.show(len_prefix,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "experimental-documentary",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapValues(line):\n",
    "    seq=line[0]\n",
    "    final=''\n",
    "    #voc[seq]=line[1]\n",
    "    for i,window in enumerate(seq):\n",
    "        if i>0:\n",
    "            final+='-'\n",
    "        for j,el in enumerate(window):\n",
    "            if j>0:\n",
    "                final+=','\n",
    "            final+=el\n",
    "    final+=(';'+str(line[1])+';'+str(i))\n",
    "    return final  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sunset-indonesia",
   "metadata": {},
   "outputs": [],
   "source": [
    "mapDict=giveT0.map(mapValues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "forward-tongue",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mapDict.collect()\n",
    "\n",
    "# ['Piena_T0_0;19530;0',\n",
    "#  'Vuota_T0_0;31245;0',\n",
    "#  'Vuota_T0_0-Vuota_T1_0;19880;1',\n",
    "#  'Vuota_T0_0,Piena_T0_0;76;0',...\n",
    "#  'Vuota_T0_0,Vuota_T0_2,Piena_T0_3;3109;0',\n",
    "#  'Piena_T0_0,Vuota_T0_3-Vuota_T2_3;2451;1', ...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "multiple-found",
   "metadata": {},
   "outputs": [],
   "source": [
    "li=mapDict.collect()\n",
    "voc={}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "substantial-parent",
   "metadata": {},
   "outputs": [],
   "source": [
    "for el in li:\n",
    "    splits=el.split(';')\n",
    "    voc[splits[0]]=int(splits[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incomplete-machine",
   "metadata": {},
   "outputs": [],
   "source": [
    "#voc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "coastal-shark",
   "metadata": {},
   "outputs": [],
   "source": [
    "repeated_el_window=0\n",
    "for el in voc.keys():\n",
    "    flag_rep=False\n",
    "    windows=el.split('-')\n",
    "    for w in windows:\n",
    "        tot_items=len(w.split(','))\n",
    "        set_items=len(set(w.split(',')))\n",
    "        if tot_items!=set_items:\n",
    "            repeated_el_window+=1\n",
    "            break\n",
    "repeated_el_window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prepared-addition",
   "metadata": {},
   "outputs": [],
   "source": [
    "voc_supports={}\n",
    "for el in voc.keys():    \n",
    "    if len(el.split('-'))>1:        \n",
    "        num=int(voc[el])       \n",
    "        string=''\n",
    "        tot=el.split('-')[:-1]\n",
    "        for k,station in enumerate(tot):\n",
    "            if k>0:\n",
    "                string+='-'\n",
    "            string+=station\n",
    "        #print(string)\n",
    "        den=int(voc[string])\n",
    "        #nella seguente riga viene riportato supporto - supportCount\n",
    "        voc_supports[el]=str(num/den)+' - '+str(voc[el])\n",
    "keys=list(voc_supports.keys())\n",
    "values=list(voc_supports.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "desperate-national",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sort vocabulary by decreasing values and sort within each window\n",
    "for key in voc_supports:    \n",
    "    splitted=key.split('-')      \n",
    "    splitted.sort()\n",
    "# voc_supports = dict(sorted(voc_supports.items(), key=operator.itemgetter(1),reverse=True))\n",
    "voc_supports = dict(sorted(voc_supports.items(), \n",
    "                           key=lambda v: (float(v[1].split(' - ')[0]), int(v[1].split(' - ')[1])),\n",
    "                           reverse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "working-deposit",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(voc_supports)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "handy-alfred",
   "metadata": {},
   "outputs": [],
   "source": [
    "# output_file='results_..._ordered_by_confidence.txt'\n",
    "file = open(output_file, \"w\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "concerned-vietnamese",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_pattern=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mental-sleeping",
   "metadata": {},
   "outputs": [],
   "source": [
    "for el in voc_supports:\n",
    "    key_list=[]\n",
    "    for e in el.split('-'):\n",
    "        key_list.append([e])\n",
    "    list_pattern.append([[key_list], [voc_supports[el]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "inclusive-rating",
   "metadata": {},
   "outputs": [],
   "source": [
    "file.write('Pattern, Confidence-Frequence'+'\\n')\n",
    "file.write(f'Total number of input patterns: {len(voc_supports)}'+'\\n')\n",
    "for el in list_pattern:  \n",
    "    file.write(str(el)+ '\\n')    \n",
    "file.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "monthly-valentine",
   "metadata": {},
   "outputs": [],
   "source": [
    "# output_file4='results_..._ordered_by_support.txt'\n",
    "file4 = open(output_file4, \"w\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "preliminary-elimination",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list_pattern[1][1][0].split(\" - \")[1]\n",
    "# list_pattern[1][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wrapped-interface",
   "metadata": {},
   "outputs": [],
   "source": [
    "#order list fist by support and then by confidence\n",
    "list_ordered_by_support = sorted(list_pattern,\n",
    "                                 key=lambda v: (int(v[1][0].split(\" - \")[1]), float(v[1][0].split(\" - \")[0])),\n",
    "                                 reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "secure-victoria",
   "metadata": {},
   "outputs": [],
   "source": [
    "file4.write('Pattern, Confidence-Frequence'+'\\n')\n",
    "file4.write(f'Total number of input patterns: {len(voc_supports)}'+'\\n')\n",
    "for el in list_ordered_by_support:  \n",
    "    file4.write(str(el)+ '\\n')    \n",
    "file4.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "twelve-agriculture",
   "metadata": {},
   "outputs": [],
   "source": [
    "tot_list=[]\n",
    "for i, j in zip(keys,values):\n",
    "    key_list=[]\n",
    "    for el in i.split('-'):\n",
    "        key_list.append([el])\n",
    "    tot_list.append([[key_list], [j]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "champion-converter",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list1 = [1,2,3]\n",
    "# list2 = [\"a\",\"b\",\"c\"]\n",
    "# for i, j in zip(list1 ,list2):\n",
    "#     print (f\"i: {i}, j: {j}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "colonial-extraction",
   "metadata": {},
   "outputs": [],
   "source": [
    "confidence=[]\n",
    "# print(values)\n",
    "for el in values:\n",
    "#     if \"e\" in el:\n",
    "#         v= el.split('-')[0] +\"-\"+ el.split('-')[1]\n",
    "#         val=round(float(v),2)\n",
    "#     else:\n",
    "    val=round(float(el.split(' - ')[0]),2)\n",
    "    confidence.append(val)\n",
    "#confidence    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cross-operator",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(confidence)\n",
    "plt.xlabel('Confidence')\n",
    "plt.ylabel('Number of patterns')\n",
    "plt.title('3 spatial thresholds, 3 time thresholds, 1000 meters, support=0')\n",
    "plt.xlim(0,1)\n",
    "plt.savefig(img_support)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "turned-boston",
   "metadata": {},
   "outputs": [],
   "source": [
    "#how many min, max, avg items there are\n",
    "num_items=0\n",
    "num_freq_items=0\n",
    "min_items=sys.maxsize\n",
    "max_items=0\n",
    "avg_items=0\n",
    "\n",
    "num_freq_window=0\n",
    "tot_window=0\n",
    "num_patterns=0\n",
    "min_window=sys.maxsize\n",
    "max_window=0\n",
    "avg_window=0\n",
    "flag_piena=False\n",
    "flag_quasi_piena=False\n",
    "\n",
    "list_piena_quasi_piena=[]\n",
    "\n",
    "for key in voc_supports.keys():\n",
    "    flag_piena=False\n",
    "    flag_quasi_piena=False    \n",
    "    string=''\n",
    "    \n",
    "    #statistics for the windows\n",
    "    if '-' in key:\n",
    "        patterns=len(key.split('-'))\n",
    "        tot_window+=patterns      \n",
    "    else:\n",
    "        tot_window+=1\n",
    "        patterns=1\n",
    "    num_patterns+=1\n",
    "    \n",
    "    #statistics for the ìtems\n",
    "    if ('-' in key and ',' in key):\n",
    "        new_key=key       \n",
    "        new_key=new_key.replace(',', '-')               \n",
    "        items=new_key.split('-')\n",
    "        for el in items:\n",
    "            string+=el.split('_')[0]\n",
    "            if el.split('_')[0].startswith('Piena') and flag_piena==False:\n",
    "                flag_piena=True\n",
    "            elif el.split('_')[0].startswith('Vuota') and flag_quasi_piena==False:\n",
    "                flag_quasi_piena=True\n",
    "      \n",
    "       \n",
    "    elif ('-' in key):\n",
    "        items=key.split('-')\n",
    "        for el in items:\n",
    "            string+=el.split('_')[0]\n",
    "            if el.split('_')[0].startswith('Piena') and flag_piena==False:\n",
    "                flag_piena=True\n",
    "            elif el.split('_')[0].startswith('Vuota') and flag_quasi_piena==False:\n",
    "                flag_quasi_piena=True\n",
    "       \n",
    "    elif ( ',' in key):\n",
    "        items=key.split(',')\n",
    "        for el in items:\n",
    "            string+=el.split('_')[0]\n",
    "            if el.split('_')[0].startswith('Piena') and flag_piena==False:\n",
    "                flag_piena=True\n",
    "            elif el.split('_')[0].startswith('Vuota') and flag_quasi_piena==False:\n",
    "                flag_quasi_piena=True\n",
    "      \n",
    "    else:\n",
    "        items=list(key)\n",
    "        for el in items:\n",
    "            string+=el.split('_')[0]\n",
    "            if el.split('_')[0].startswith('Piena') and flag_piena==False:\n",
    "                flag_piena=True\n",
    "            elif el.split('_')[0].startswith('Vuota') and flag_quasi_piena==False:\n",
    "                flag_quasi_piena=True\n",
    "    if (flag_quasi_piena==True and flag_piena==True):       \n",
    "        #list_piena_quasi_piena.append([key])\n",
    "        key_list=[]            \n",
    "        for e in key.split('-'):\n",
    "            key_list.append([e])\n",
    "        list_piena_quasi_piena.append([[key_list], [voc_supports[key]]])\n",
    "\n",
    "    q_items=len(items) \n",
    "    if min_items>q_items:\n",
    "        min_items=q_items\n",
    "    if max_items<q_items:\n",
    "        max_items=q_items\n",
    "    freq=int(voc_supports[key].split('-')[1])\n",
    "    num_items+=freq \n",
    "    num_freq_items+=freq*q_items\n",
    "    \n",
    "    if min_window> patterns:\n",
    "        min_window=patterns\n",
    "    if max_window<patterns:\n",
    "        max_window=patterns    \n",
    "    num_freq_window+=freq*patterns  \n",
    "\n",
    "avg_window=0\n",
    "if num_items!=0:\n",
    "    avg_items=float(num_freq_items)/float(num_items)    \n",
    "    print(f'The average number of items is: {avg_items}')    \n",
    "    avg_window=float(num_freq_window)/ float(num_items)\n",
    "    print(f'The average number of windows is: {avg_window}')\n",
    "else:\n",
    "    print(f'The average number of windows is: {avg_window}')\n",
    "    \n",
    "print(f'The minimum number of items is: {min_items}')\n",
    "print(f'The maximum number of items is: {max_items}')\n",
    "print(f'The minimum number of windows is: {min_window}')\n",
    "print(f'The maximum number of windows is: {max_window}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "twelve-piano",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('FILTER PATTERNS WITH AT LEAST 1 EVENT VUOTA AND 1 EVENT PIENA ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "blocked-short",
   "metadata": {},
   "outputs": [],
   "source": [
    "# output_file2='results_..._Vuota_Piena_ordered_by_confidence.txt'\n",
    "file2 = open(output_file2, \"w\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "opened-answer",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lung_piena_quasipiena=0\n",
    "if len(list_piena_quasi_piena)!=0:\n",
    "    df_supports=sc.parallelize(list_piena_quasi_piena).toDF().withColumnRenamed('_1','sequence')\n",
    "    df_supports=df_supports.withColumnRenamed('_2','confidence-freq')\n",
    "    lung_piena_quasipiena=df_supports.count() \n",
    "#     df_supports.show(lung_piena_quasipiena,False)\n",
    "#     print(lung_piena_quasipiena)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sunrise-china",
   "metadata": {},
   "outputs": [],
   "source": [
    "#list_piena_quasi_piena"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acting-extension",
   "metadata": {},
   "outputs": [],
   "source": [
    "file2.write('Pattern, Confidence-Frequence'+'\\n')\n",
    "file2.write(f'Total number of input patterns: {len(list_piena_quasi_piena)}'+'\\n')\n",
    "if len(list_piena_quasi_piena)!=0:\n",
    "    for el in list_piena_quasi_piena:\n",
    "        #print(el)\n",
    "        file2.write(str(el)+'\\n')\n",
    "file2.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "proud-relative",
   "metadata": {},
   "outputs": [],
   "source": [
    "# output_file5='results_..._Vuota_Piena_ordered_by_support.txt'\n",
    "file5 = open(output_file5, \"w\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ranging-cement",
   "metadata": {},
   "outputs": [],
   "source": [
    "#order list fist by support an then by confidence\n",
    "list_piena_quasi_piena_ordered_by_support = sorted(list_piena_quasi_piena,\n",
    "                                                   key=lambda v: (int(v[1][0].split(\" - \")[1]), float(v[1][0].split(\" - \")[0])),\n",
    "                                                   reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fatal-trust",
   "metadata": {},
   "outputs": [],
   "source": [
    "file5.write('Pattern, Confidence-Frequence'+'\\n')\n",
    "file5.write(f'Total number of input patterns: {len(list_piena_quasi_piena_ordered_by_support)}'+'\\n')\n",
    "if len(list_piena_quasi_piena_ordered_by_support)!=0:\n",
    "    for el in list_piena_quasi_piena_ordered_by_support:\n",
    "        #print(el)\n",
    "        file5.write(str(el)+'\\n')\n",
    "file5.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "relative-rover",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('FILTER PATTERNS WITH AT LEAST 1 T0, DELTA S=0 AND AT LEAST 1 PATTERN WITH AT LEAST 1 PATTERN WITH DELTA S DIFFERENT FROM 0 AND DELTA T DIFFERENT FROM 0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bigger-honor",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_influenze=[]\n",
    "for el in voc_supports.keys():\n",
    "    delta_spaziale=False\n",
    "    if ('-' in el):\n",
    "        all_windows_list=el.split('-')\n",
    "        if ('T0_0' in all_windows_list[0] ):\n",
    "            for cons_window in all_windows_list[1::]:\n",
    "                if ',' in cons_window:\n",
    "                    for item in cons_window.split(','):                       \n",
    "                        act_delta=int(item.split('_')[2])\n",
    "                        if act_delta!=0:\n",
    "                            delta_spaziale=True\n",
    "    if delta_spaziale==True:        \n",
    "        key_list=[]            \n",
    "        for e in el.split('-'):\n",
    "            key_list.append([e])\n",
    "        list_influenze.append([[key_list], [voc_supports[el]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "designing-optimization",
   "metadata": {},
   "outputs": [],
   "source": [
    "# output_file3='results_..._diff_delta_ordered_by_confidence.txt'\n",
    "file3 = open(output_file3, \"w\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "furnished-meter",
   "metadata": {},
   "outputs": [],
   "source": [
    "lung_different_time_space=0\n",
    "if len(list_influenze)!=0:\n",
    "    df_supports=sc.parallelize(list_influenze).toDF().withColumnRenamed('_1','sequence')\n",
    "    df_supports=df_supports.withColumnRenamed('_2','confidence-freq')\n",
    "    lung_different_time_space=df_supports.count() \n",
    "    #df_supports.show(lung_different_time_space,False)\n",
    "    #print(lung_different_time_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "progressive-poison",
   "metadata": {},
   "outputs": [],
   "source": [
    "file3.write('Pattern, Confidence-Frequence'+'\\n')\n",
    "file3.write(f'Total number of input patterns: {len(list_influenze)}'+'\\n')\n",
    "if len(list_influenze)!=0:\n",
    "    for el in list_influenze:       \n",
    "        file3.write(str(el)+'\\n')\n",
    "file3.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "addressed-label",
   "metadata": {},
   "outputs": [],
   "source": [
    "# output_file6='results_..._diff_delta_ordered_by_support.txt'\n",
    "file6 = open(output_file6, \"w\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "directed-florence",
   "metadata": {},
   "outputs": [],
   "source": [
    "#order list first by support and then by confidence\n",
    "list_influenze_ordered_by_support = sorted(list_influenze,\n",
    "                                           key=lambda v: (int(v[1][0].split(\" - \")[1]), float(v[1][0].split(\" - \")[0])),\n",
    "                                           reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "executive-mineral",
   "metadata": {},
   "outputs": [],
   "source": [
    "file6.write('Pattern, Confidence-Frequence'+'\\n')\n",
    "file6.write(f'Total number of input patterns: {len(list_influenze_ordered_by_support)}'+'\\n')\n",
    "if len(list_influenze_ordered_by_support)!=0:\n",
    "    for el in list_influenze_ordered_by_support:       \n",
    "        file6.write(str(el)+'\\n')\n",
    "file6.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "manufactured-patrick",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'The number of patterns in the pre-filter is: {len_prefix}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "supposed-bracelet",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'The number of items after the filter with at least 2 windows and at least a T0 and delta 0 is: {len(voc_supports)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "heard-guidance",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('STATISTICS about sequences with at least 2 windows_T0_delta0')\n",
    "print(f'The average number of windows is: {avg_window}')\n",
    "print(f'The minimum number of windows is: {min_window}')\n",
    "print(f'The maximum number of windows is: {max_window}')\n",
    "print(f'The average number of items is: {avg_items}')\n",
    "print(f'The minimum number of items is: {min_items}')\n",
    "print(f'The maximum number of items is: {max_items}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "north-geology",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'The number of patterns in which there is at least one item that repeats within a window is: {repeated_el_window} ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "italian-scholarship",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'The number of patterns with at least 1 event QuasiPiena and 1 event Piena is: {len(list_piena_quasi_piena)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "moderate-minute",
   "metadata": {},
   "outputs": [],
   "source": [
    " print(f'The number of patterns with at least 1 T0, DELTA S=0 and at least 1 pattern with at least 1 pattern with DELTA S different from 0 and DELTA T different from 0 is: {lung_different_time_space}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "analyzed-reason",
   "metadata": {},
   "outputs": [],
   "source": [
    "end = time.time()\n",
    "print(f'The time of execution is: {end-start} s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aggressive-advocacy",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "essential-chase",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark (Yarn)",
   "language": "python",
   "name": "pyspark_yarn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
