{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "prospective-memory",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "geological-remove",
   "metadata": {},
   "outputs": [],
   "source": [
    "#IMPORTS\n",
    "from datetime import datetime\n",
    "from pyspark.ml.fpm import PrefixSpan\n",
    "from pyspark.sql import Row\n",
    "import pyspark.sql.functions as F\n",
    "from math import sin, cos, sqrt, atan2, radians \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import operator\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "invalid-hurricane",
   "metadata": {},
   "outputs": [],
   "source": [
    "#parameters\n",
    "interval=30 #time window\n",
    "th=1 #distance\n",
    "window_size=5 #window size\n",
    "maxDelta=3 #how many delta\n",
    "support=0.05\n",
    "support_str=\"005\"\n",
    "station_status= \"StateChange_Normal_almostEmpty\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "falling-privilege",
   "metadata": {},
   "outputs": [],
   "source": [
    "#FILES\n",
    "inputPath  = \"file:///home/bigdata-01QYD/s270240/bike_sharing/filtered_status.csv\"\n",
    "STATION_PATH=\"file:///home/bigdata-01QYD/s270240/bike_sharing/station.csv\"\n",
    "#save file of first filter\n",
    "output_file=f'../Results_extraction/{station_status}/{station_status}_{interval}_{int(th*1000)}_{support_str}({window_size}-{maxDelta})/results_{int(th*1000)}_{support_str}_ordered_by_confidence.txt'\n",
    "# output_file2=f'../Results_extraction/StateChange_Full_almostFull/StateChange_Full_almostFull_{interval}_{int(th*1000)}_{support}({window_size}-{maxDelta})/results_{int(th*1000)}_{support}_QuasiVuota_Vuota_ordered_by_confidence.txt'\n",
    "# output_file3=f'../Results_extraction/StateChange_Full_almostFull/StateChange_Full_almostFull_{interval}_{int(th*1000)}_{support}({window_size}-{maxDelta})/results_{int(th*1000)}_{support}_diff_delta_ordered_by_confidence.txt'\n",
    "output_file4=f'../Results_extraction/{station_status}/{station_status}_{interval}_{int(th*1000)}_{support_str}({window_size}-{maxDelta})/results_{int(th*1000)}_{support_str}_ordered_by_support.txt'\n",
    "# output_file5=f'../Results_extraction/StateChange_Full_almostFull/StateChange_Full_almostFull_{interval}_{int(th*1000)}_{support}({window_size}-{maxDelta})/results_{int(th*1000)}_{support}_QuasiVuota_Vuota_ordered_by_support.txt'\n",
    "# output_file6=f'../Results_extraction/StateChange_Full_almostFull/StateChange_Full_almostFull_{interval}_{int(th*1000)}_{support}({window_size}-{maxDelta})/results_{int(th*1000)}_{support}_diff_delta_ordered_by_support.txt'\n",
    "img_support=f'../Results_extraction/{station_status}/{station_status}_{interval}_{int(th*1000)}_{support_str}({window_size}-{maxDelta})/{window_size}-{maxDelta}-{int(th*1000)}-{support_str}.jpg'\n",
    "\n",
    "# output_file=f'./results_withNormal_{int(th*1000)}_{support}_ordered_by_confidence.txt'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "adverse-charleston",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputDF = spark.read.format(\"csv\")\\\n",
    ".option(\"delimiter\", \",\")\\\n",
    ".option(\"header\", True)\\\n",
    ".option(\"inferSchema\", True).load(inputPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "english-geneva",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputDF=inputDF.filter(\"docks_available is not null\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "optical-music",
   "metadata": {},
   "outputs": [],
   "source": [
    "#filter for these fields\n",
    "filteredDF = inputDF.filter(\"docks_available<>0 OR bikes_available<>0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "bulgarian-soccer",
   "metadata": {},
   "outputs": [],
   "source": [
    "#determine if the station is normal or almost empty\n",
    "def stateFunction(docks_available,bikes_available):\n",
    "    if bikes_available<=2: # almostEmpty\n",
    "        return 1\n",
    "    else:         #normal\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "decent-drinking",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.stateFunction(docks_available, bikes_available)>"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.udf.register(\"state\", stateFunction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "vocal-adams",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getInt(station):\n",
    "    return (station)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "handled-water",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.getInt(station)>"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.udf.register(\"intValue\", getInt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "sought-frontier",
   "metadata": {},
   "outputs": [],
   "source": [
    "getStatusDF = filteredDF.selectExpr(\"station_id\",\"time\", \"state(docks_available,bikes_available) as status\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "digital-smith",
   "metadata": {},
   "outputs": [],
   "source": [
    "# getStatusDF.show(100,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "opening-overall",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a view\n",
    "getStatusDF.createOrReplaceTempView(\"readings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "grave-construction",
   "metadata": {},
   "outputs": [],
   "source": [
    "#select station, year, month, day, hour, minute, status ordered by time\n",
    "ss=spark.sql(\"\"\"SELECT  station_id , YEAR(time) as year, MONTH(time) as month, DAY(time) as day, HOUR(time)as hour, MINUTE(time) as minute, status\n",
    "FROM readings\n",
    "GROUP BY station_id, year, month, day,hour,minute, status\n",
    "ORDER BY  station_id,year, month,day, hour,minute\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "thrown-album",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create rdd and group into interval\n",
    "my_rdd=ss.rdd.map(tuple)\n",
    "rdd=my_rdd.map(lambda line: (line[0],line[1],line[2], line[3], line[4], int(line[5]/interval), line[6])).distinct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "documented-castle",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rdd.collect()\n",
    "\n",
    "# [(6, 2013, 12, 31, 1, 1, '2'),\n",
    "#  (6, 2014, 1, 14, 22, 1, '2'),\n",
    "#  (6, 2014, 1, 16, 0, 1, '2'),\n",
    "#  (6, 2014, 1, 19, 6, 0, '2'),\n",
    "#  (6, 2014, 1, 23, 21, 1, '2'),\n",
    "#  (6, 2014, 1, 26, 10, 1, '2'),\n",
    "#  (6, 2014, 1, 30, 14, 1, '2'), ... ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "turned-order",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get distinct stations to calculate distances\n",
    "id_stations=rdd.map(lambda line: line[0]).distinct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "cosmetic-warehouse",
   "metadata": {},
   "outputs": [],
   "source": [
    "tot_id_stations=id_stations.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "surgical-hudson",
   "metadata": {},
   "outputs": [],
   "source": [
    "#obtain timestamp and info\n",
    "def getMap2(line):\n",
    "    id_station=str(line[0])\n",
    "    year=int(line[1])\n",
    "    month=int(line[2])\n",
    "    day=int(line[3])\n",
    "    hour=int(line[4])\n",
    "    minute=int(line[5])   \n",
    "    timestamp= datetime(year,month, day, hour, minute)  \n",
    "    status=int(line[6])\n",
    "    info=str(id_station)+' '+str(status)\n",
    "    return (timestamp, info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "australian-exhaust",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_map2=rdd.map(getMap2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "passive-tenant",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_map2.collect()\n",
    "\n",
    "# [(datetime.datetime(2014, 3, 31, 18, 1), '11 2'),\n",
    "#  (datetime.datetime(2014, 4, 6, 21, 0), '11 2'),\n",
    "#  (datetime.datetime(2014, 4, 10, 22, 1), '11 2'),\n",
    "#  (datetime.datetime(2014, 4, 12, 8, 1), '11 2'),\n",
    "#  (datetime.datetime(2014, 4, 13, 8, 0), '11 2'), ... ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "regulated-edinburgh",
   "metadata": {},
   "outputs": [],
   "source": [
    "reduceK2=get_map2.reduceByKey(lambda l1,l2 :(l1+','+l2)).sortByKey()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "blessed-startup",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reduceK2.collect()\n",
    "\n",
    "# [(datetime.datetime(2013, 8, 29, 10, 0), '59 0,67 0,4 1,13 1,67 1,6 0,2 0,7 1,16 0,70 1,70 0'),\n",
    "#  (datetime.datetime(2013, 8, 29, 10, 1), '67 0,7 1,16 0,59 0,13 1,67 1,70 0,2 0,7 0,69 0,4 1,13 0,73 0'),\n",
    "#  (datetime.datetime(2013, 8, 29, 11, 0), '67 1,2 0,4 0,73 0,67 0,69 0,16 0,70 0,4 1'),\n",
    "#  (datetime.datetime(2013, 8, 29, 11, 1), '16 0,73 0,2 0,69 0,67 0'),\n",
    "#  (datetime.datetime(2013, 8, 29, 12, 0), '69 1,69 0,16 0,2 0,73 0'), ... ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "valuable-start",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_tot=reduceK2.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "strange-raleigh",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list_tot[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "detailed-child",
   "metadata": {},
   "outputs": [],
   "source": [
    "voc_status={}\n",
    "voc_timestamp={}\n",
    "list_filtered=[]\n",
    "\n",
    "voc_tmp={}\n",
    "\n",
    "# previous_list=[]\n",
    "# updated_list=[]\n",
    "\n",
    "for i, el in enumerate(list_tot):\n",
    "    #updated_list=[]    \n",
    "    information=el[1]   \n",
    "    tmp_list_station='' \n",
    "    voc_tmp={}\n",
    "    \n",
    "    for info in information.split(','):\n",
    "#         print()\n",
    "#         print(info)\n",
    "        station=str(info.split(' ')[0])\n",
    "            \n",
    "        status=int(info.split(' ')[1])\n",
    "        if  station not in voc_status:\n",
    "#             print (f\"station: {station} NOT PRESENT\")\n",
    "            voc_status[station]=status\n",
    "            tmp_time= el[0].replace(minute=el[0].minute*interval) #riconverto nei minuti originali \n",
    "            voc_timestamp[station]=tmp_time\n",
    "#             print (f\"actual_time: {voc_timestamp[station]}\")\n",
    "            stringa=''\n",
    "            if status==1:\n",
    "                stringa='QuasiVuota'\n",
    "            else:\n",
    "                stringa='Normal'    \n",
    "            tmp_list_station+=','+station+'_'+ stringa \n",
    "#             print(tmp_list_station)\n",
    "            #updated_list.append(station)\n",
    "                  \n",
    "        else:\n",
    "#             print (f\"station: {station} PRESENT\")\n",
    "            times=voc_timestamp[station]\n",
    "#             print (f\"times: {times}\")\n",
    "            tmp_time= el[0].replace(minute=el[0].minute*interval) #riconverto nei minuti originali\n",
    "            actual_time=tmp_time\n",
    "#             print (f\"actual_time: {actual_time}\")\n",
    "#             print (f\"old satus: {voc_status[station]}\")\n",
    "#             print (f\"new satus: {status}\")\n",
    "            \n",
    "            if times == actual_time:    #inside same timeslot\n",
    "                if status != voc_status[station]:   # if status changed   \n",
    "#                     print(\"STATUS CHANGED INSIDE THE SAME TIMESLOT!!!!!\")\n",
    "                    voc_status[station]=3  #set flag of status changed inside same timeslot\n",
    "                    stringa=''\n",
    "                    if status==1:\n",
    "                        stringa='QuasiVuota'\n",
    "                    else:\n",
    "                        stringa='Normal'    \n",
    "                    tmp_list_station+=','+station+'_'+ stringa \n",
    "#                     print(tmp_list_station)\n",
    "                    \n",
    "            else:            # different timeslot\n",
    "#                 print(f\"actual_time.minute-times.minute: {int((actual_time-times).total_seconds() / 60.0)}\")\n",
    "#                 if ((actual_time.hour == times.hour and actual_time.minute-times.minute==1) or\n",
    "#                     (actual_time.hour-times.hour==1 and actual_time.minute-times.minute==-1)):\n",
    "                \n",
    "                if (int((actual_time-times).total_seconds() / 60.0)== interval): # after less than 30 minutes\n",
    "#                     print(f\"after <= 30 minutes\")\n",
    "                    if voc_status[station]==3:         #if status changed inside the same old timeslot        \n",
    "#                         print(\"STATUS CHANGED INSIDE THE SAME OLD TIMESLOT!!!!!\")\n",
    "                        voc_status[station]=status     #update old status with new one\n",
    "                        voc_timestamp[station]=actual_time      #update old timestamp with new one\n",
    "                        stringa=''\n",
    "                        if status==1:\n",
    "                            stringa='QuasiVuota'\n",
    "                        else:\n",
    "                            stringa='Normal'\n",
    "                        tmp_list_station+=','+station+'_'+ stringa \n",
    "#                         print(tmp_list_station)\n",
    "                        #updated_list.append(station)\n",
    "                        \n",
    "                    elif voc_status[station] != status:  # if status changed between old and new timeslot \n",
    "#                         print(\"STATUS CHANGED!!!!!\")\n",
    "                        voc_status[station]=status     #update old status with new one\n",
    "                        voc_timestamp[station]=actual_time    #update old timestamp with new one\n",
    "                        stringa=''\n",
    "                        if status==1:\n",
    "                            stringa='QuasiVuota'\n",
    "                        else :\n",
    "                            stringa='Normal'\n",
    "                        tmp_list_station+=','+station+'_'+ stringa \n",
    "#                         print(tmp_list_station)\n",
    "                        \n",
    "                        if station in voc_tmp:\n",
    "                            if voc_tmp[station]==1:\n",
    "                                 tmp_list_station+=','+station+'_QuasiVuota'\n",
    "                            elif voc_tmp[station]==0:\n",
    "                                 tmp_list_station+=','+station+'_Normal'\n",
    "                            \n",
    "#                             print(tmp_list_station)\n",
    "                        \n",
    "                        \n",
    "                    elif  voc_status[station] == status: # if status did not change between old and new timeslot\n",
    "#                         print(\"STATUS DID NOT CHANGE!!!!!\")\n",
    "                        voc_tmp[station]=status\n",
    "                        tmp_list_station+=','+station+'_NoChange'\n",
    "#                         print(tmp_list_station)\n",
    "#                         voc_timestamp[station]=actual_time     #!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "                    \n",
    "                # after more than 30 minutes        \n",
    "                else:\n",
    "#                     print(f\"after >= 30 minutes\")\n",
    "                    if voc_status[station] != status:\n",
    "                        stringa=''\n",
    "                        if status==1:\n",
    "                            stringa='QuasiVuota'\n",
    "                        else :\n",
    "                            stringa='Normal'\n",
    "                        tmp_list_station+=','+station+'_'+ stringa \n",
    "#                         print(tmp_list_station)\n",
    "                    else:\n",
    "                        tmp_list_station+=','+station+'_NoChange'\n",
    "#                     print (f\"REMOVING STATION {station}\")    \n",
    "#                     del voc_status[station]\n",
    "#                     del voc_timestamp[station]\n",
    "                    voc_timestamp[station]=actual_time\n",
    "                \n",
    "    \n",
    "    list_filtered.append((el[0],tmp_list_station[1::]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "aquatic-philip",
   "metadata": {},
   "outputs": [],
   "source": [
    "reduceK=spark.sparkContext.parallelize(list_filtered)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "reserved-guitar",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reduceK.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "instrumental-athens",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_df=reduceK.toDF()\n",
    "#my_df.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "treated-compilation",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_df.createOrReplaceTempView(\"view\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "clean-pepper",
   "metadata": {},
   "outputs": [],
   "source": [
    "s2=spark.sql(\"\"\"SELECT ROW_NUMBER() OVER(ORDER BY _1,_2) as id ,_1, _2\n",
    "FROM view \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "respected-ticket",
   "metadata": {},
   "outputs": [],
   "source": [
    "#identifier of the timestamp, info\n",
    "rdd_scheme=s2.rdd.map(tuple).map(lambda line: (line[0], line[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "eastern-database",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rdd_scheme.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "encouraging-kruger",
   "metadata": {},
   "outputs": [],
   "source": [
    "#obtain window, station-status\n",
    "def giveSplit(line):   \n",
    "    id_window=( int(line[0] ))\n",
    "    lista=[]    \n",
    "    counter=id_window    \n",
    "    while counter>=1:\n",
    "        lista.append(('Window '+str(counter),(line[1])))\n",
    "        counter=counter-1\n",
    "        if (id_window-counter)==window_size:\n",
    "            return lista  \n",
    "    return lista"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "patent-context",
   "metadata": {},
   "outputs": [],
   "source": [
    "mapData=rdd_scheme.flatMap(giveSplit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "mineral-audio",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mapData.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "continuing-madness",
   "metadata": {},
   "outputs": [],
   "source": [
    "filteredD=mapData.filter(lambda line: line[1]!='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "original-publicity",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filteredD.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "flush-spoke",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each window get all info\n",
    "all_keys=filteredD.reduceByKey(lambda l1,l2:(l1+'-'+l2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "numerous-twins",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_keys.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "raising-russian",
   "metadata": {},
   "outputs": [],
   "source": [
    "#finestra temporale\n",
    "def reduceKeys(line):   \n",
    "    lista=[]\n",
    "    #lista.append(line[0])\n",
    "    line_split=line[1].split(\"-\")\n",
    "    #return line_split[0]\n",
    "    count=len(line_split)\n",
    "    tot=[]\n",
    "    for val in range(count):\n",
    "        li=[]\n",
    "        stations=line_split[val].split(',')\n",
    "        for st in stations:\n",
    "            if st.split('_')[1]!=\"*\":\n",
    "                all_string_st=st.split('_')[0]+'_'+'T'+str(val)+'_'+st.split('_')[1]\n",
    "                li.append(all_string_st)\n",
    "        tot.append(li)\n",
    "    lista.append((line[0],(tot))) \n",
    "    return lista"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "encouraging-heaven",
   "metadata": {},
   "outputs": [],
   "source": [
    "windows=all_keys.flatMap(reduceKeys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "defined-royalty",
   "metadata": {},
   "outputs": [],
   "source": [
    "# windows.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "pediatric-exclusive",
   "metadata": {},
   "outputs": [],
   "source": [
    "#filtering NoChange events\n",
    "def filterNormal(line):\n",
    "    complete_list = []\n",
    "    for list_el in line[1]:\n",
    "        event_list = copy.deepcopy(list_el)\n",
    "        for el in list_el:\n",
    "            if \"NoChange\"in el:\n",
    "                event_list.remove(el)\n",
    "#         new_event_list = \",\".join(event_list)\n",
    "        complete_list.append(event_list)\n",
    "    return (line[0], complete_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "removable-career",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove noChange events\n",
    "\n",
    "windows_withoutNoChange = windows.map(filterNormal)\n",
    "# windows_withoutNoChange.collect()\n",
    "\n",
    "# [('Window 1',\n",
    "#   [[],\n",
    "#    ['60_T1_QuasiPiena', '64_T1_QuasiPiena', '45_T1_Piena', '45_T1_QuasiPiena'],\n",
    "#    ['45_T2_Piena', '64_T2_Piena', '64_T2_QuasiPiena', '45_T2_QuasiPiena']]),\n",
    "#  ('Window 2',\n",
    "#   [['60_T0_QuasiPiena', '64_T0_QuasiPiena', '45_T0_Piena', '45_T0_QuasiPiena'],\n",
    "#    ['45_T1_Piena', '64_T1_Piena', '64_T1_QuasiPiena', '45_T1_QuasiPiena'],\n",
    "#    ['64_T2_QuasiPiena', '35_T2_QuasiPiena']]),... ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "grand-netscape",
   "metadata": {},
   "outputs": [],
   "source": [
    "#filtering empty list ''\n",
    "def filterEmptyList(line):\n",
    "    filtered_list = list(filter(None, line[1]))\n",
    "    return (line[0], filtered_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "current-request",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_windows = windows_withoutNoChange.map(filterEmptyList)\n",
    "# filtered_windows.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "speaking-moore",
   "metadata": {},
   "outputs": [],
   "source": [
    "#filtering empty window\n",
    "filtered_windows2 = filtered_windows.filter(lambda line: line[1])\n",
    "# filtered_windows2.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "impaired-spirituality",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save station file\n",
    "stationsDF = spark.read.format(\"csv\")\\\n",
    ".option(\"delimiter\", \",\")\\\n",
    ".option(\"header\", True)\\\n",
    ".option(\"inferSchema\", True).load(STATION_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "parallel-champion",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get only rows interested: only the used stations \n",
    "necessary_rows=stationsDF.filter(F.col(\"id\").isin(tot_id_stations)).sort(\"id\").rdd.map(tuple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "weekly-giant",
   "metadata": {},
   "outputs": [],
   "source": [
    "#necessary_rows.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "spanish-updating",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get info of stations about coordinates and name\n",
    "coordinates=necessary_rows.map(lambda line: (line[0],(str(line[2])+','+str(line[3]))))\n",
    "names_stations=necessary_rows.map(lambda line: (line[0],line[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "grave-reproduction",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_coo=coordinates.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "innovative-saudi",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list_coo\n",
    "\n",
    "# [(2, '37.329732,-121.90178200000001'),\n",
    "#  (3, '37.330698,-121.888979'),\n",
    "#  (4, '37.333988,-121.894902'),\n",
    "#  (5, '37.331415,-121.8932'),\n",
    "#  (6, '37.336721000000004,-121.894074'), ...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "limiting-commander",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a dictionary in which the key is the station and value is the info about coordinates\n",
    "dic_co=coordinates.collectAsMap()\n",
    "dic_coordinates=sc.broadcast(dic_co)\n",
    "#dic_coordinates.value[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "unsigned-embassy",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to retrieve distance between 2 stations\n",
    "def getDistance(station1,station2):\n",
    "    # approximate radius of earth in km\n",
    "    R = 6373.0    \n",
    "    lat_a=float(station1.split(',')[0])\n",
    "    lat_b=float(station2.split(',')[0])\n",
    "    long_a=float(station1.split(',')[1])\n",
    "    long_b=float(station2.split(',')[1])\n",
    "    \n",
    "    lat1=radians(lat_a)\n",
    "    lat2=radians(lat_b)\n",
    "    lon1=radians(long_a)\n",
    "    lon2=radians(long_b)\n",
    "    \n",
    "    dlon = lon2 - lon1\n",
    "    dlat = lat2 - lat1\n",
    "\n",
    "    a = sin(dlat / 2)**2 + cos(lat1) * cos(lat2) * sin(dlon / 2)**2\n",
    "    c = 2 * atan2(sqrt(a), sqrt(1 - a))\n",
    "\n",
    "    distance = R * c\n",
    "    return distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "accepted-three",
   "metadata": {},
   "outputs": [],
   "source": [
    "#voc in which the key is a pair of stations and value is the distance\n",
    "voc_distances={}\n",
    "for i in range(len(list_coo)):\n",
    "    for j in range(i+1,len(list_coo)):\n",
    "        station1=list_coo[i][0]\n",
    "        station2=list_coo[j][0]\n",
    "        d_i=list_coo[i][1]\n",
    "        d_j=list_coo[j][1]\n",
    "        distance=getDistance(d_i,d_j)\n",
    "        id_stations=str(station1)+' '+str(station2)\n",
    "        voc_distances[id_stations]=distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "exciting-consensus",
   "metadata": {},
   "outputs": [],
   "source": [
    "# voc_distances['2 3']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "fantastic-firewall",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # average distance between stations\n",
    "# avg_distance=0\n",
    "# for dist in voc_distances.values():\n",
    "#     avg_distance+=dist\n",
    "# avg_distance=avg_distance/len(voc_distances)\n",
    "# #avg_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "common-theater",
   "metadata": {},
   "outputs": [],
   "source": [
    "# avg_distance_float=float(\"{:.2f}\".format(avg_distance))\n",
    "# print(f'The average distance is {avg_distance_float} Km')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "concerned-update",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set_all_stations=set()\n",
    "# for key in voc_distances:\n",
    "#     first_el=int(key.split(' ')[0])\n",
    "#     second_el=int(key.split(' ')[1])\n",
    "#     set_all_stations.add(first_el)\n",
    "#     set_all_stations.add(second_el)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "grateful-consortium",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # all stations that are at a distance smaller or equal to 1 km from station 1 as reference\n",
    "# list_stations=[]\n",
    "# count_stations=0\n",
    "# reference_station=2\n",
    "\n",
    "# for key in voc_distances:\n",
    "#     first_el=int(key.split(' ')[0])\n",
    "#     second_el=int(key.split(' ')[1])\n",
    "#     if first_el==reference_station and voc_distances[key]<=1: \n",
    "#         count_stations+=1\n",
    "#         list_stations.append(second_el)       \n",
    "#     elif second_el==reference_station and voc_distances[key]<=1:\n",
    "#         count_stations+=1\n",
    "#         list_stations.append(first_el)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "hourly-secret",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list_stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "incident-insertion",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for distance in [0.5,1,2,3,4,5,10,20,50,100]:\n",
    "#     sum_stations=0\n",
    "#     for reference_station in list(set_all_stations):\n",
    "#     #     list_stations=[]\n",
    "#         count_stations=0    \n",
    "#         for key in voc_distances:\n",
    "#             first_el=int(key.split(' ')[0])\n",
    "#             second_el=int(key.split(' ')[1])\n",
    "#             if first_el==reference_station and voc_distances[key]<=distance: \n",
    "#                 count_stations+=1\n",
    "#     #             list_stations.append(second_el)       \n",
    "#             elif second_el==reference_station and voc_distances[key]<=distance:\n",
    "#                 count_stations+=1\n",
    "#     #             list_stations.append(first_el)\n",
    "#         sum_stations+=count_stations  \n",
    "#     num_stations_Km=float(\"{:.2f}\".format(sum_stations/len(list(set_all_stations))))\n",
    "#     print(f'The average number of stations that are at a distance smaller or equal than {distance} Km is {num_stations_Km}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decimal-wallpaper",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "comprehensive-parade",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Applicazione “Delta Spaziale”\n",
    "def giveSpatialWindow(line):\n",
    "    lista=[]    \n",
    "    time0=line[1][0]    #\n",
    "    dic={}\n",
    "    \n",
    "    count_windows=len(line[1])#tot windows\n",
    "\n",
    "    for station in time0:# only first window\n",
    "        act_station=int(station.split('_')[0])\n",
    "        #lista_station=[] \n",
    "        list_tmp=[]\n",
    "        \n",
    "        #for each window\n",
    "        for i,window in enumerate(line[1]):           \n",
    "            second_lista=[]\n",
    "            #for each element of a window\n",
    "            for all_el in window :\n",
    "                #second_lista=[]\n",
    "                \n",
    "                act_all_el=int(all_el.split('_')[0])\n",
    "                time_window= all_el.split('_')[1]\n",
    "                state=all_el.split('_')[2]\n",
    "               \n",
    "                if act_station!=act_all_el:\n",
    "                    \n",
    "                    key=''\n",
    "                    if act_station<act_all_el:\n",
    "                        key=str(act_station)+' '+str(act_all_el)\n",
    "                    else:\n",
    "                        key=str(act_all_el)+' '+str(act_station)                    \n",
    "                    \n",
    "                    dist=voc_distances[key]\n",
    "                    if dist<=maxDelta*th:\n",
    "                        delta=0\n",
    "                        for d in range(1,maxDelta+1):\n",
    "                            if d*th>=dist:\n",
    "                                delta=d\n",
    "                                break                        \n",
    "                        string=state+'_'+time_window+'_'+str(delta)\n",
    "                        second_lista.append(string)\n",
    "                else:\n",
    "                    string=state+'_'+time_window+'_'+str(0)\n",
    "                    second_lista.append(string)\n",
    "                    \n",
    "            if len(second_lista)>0:\n",
    "                list_tmp.append(second_lista)\n",
    "        lista.append(((line[0]+'|'+str(act_station)),list_tmp))\n",
    "    \n",
    "    return lista"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "nearby-present",
   "metadata": {},
   "outputs": [],
   "source": [
    "spatial_app=filtered_windows2.flatMap(giveSpatialWindow)\n",
    "# print(spatial_app.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "broad-nightmare",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spatial_app.collect()\n",
    "\n",
    "# [('Window 1|59',\n",
    "#   [['QuasiVuota_T0_0', 'QuasiVuota_T0_1', 'Vuota_T0_1', 'Vuota_T0_3', 'QuasiVuota_T0_3'],\n",
    "#    ['QuasiVuota_T1_1', 'Vuota_T1_1', 'QuasiVuota_T1_3', 'QuasiVuota_T1_3', 'QuasiVuota_T1_3']]),\n",
    "#  ('Window 1|67',\n",
    "#   [['QuasiVuota_T0_1', 'QuasiVuota_T0_0', 'Vuota_T0_0', 'Vuota_T0_2', 'QuasiVuota_T0_2'],\n",
    "#    ['QuasiVuota_T1_0', 'Vuota_T1_0', 'QuasiVuota_T1_2', 'QuasiVuota_T1_2', 'QuasiVuota_T1_3']]), ...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "applicable-encounter",
   "metadata": {},
   "outputs": [],
   "source": [
    "def row_seq(line):\n",
    "    true=line[1]\n",
    "    string=Row(sequence=true)\n",
    "    return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "amazing-speed",
   "metadata": {},
   "outputs": [],
   "source": [
    "spatial=spatial_app.map(row_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "consecutive-interim",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spatial.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "deadly-fifty",
   "metadata": {},
   "outputs": [],
   "source": [
    "# seq_rdd =spatial.rdd.map(tuple)\n",
    "filtered_seq = spatial.filter(lambda line: \"T0\" in line[0][0][0])\n",
    "# filtered_seq.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "photographic-announcement",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a dataframe\n",
    "df=filtered_seq.toDF()\n",
    "# len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "flush-reset",
   "metadata": {},
   "outputs": [],
   "source": [
    "supports=[support]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "statutory-excerpt",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|sequence                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 |\n",
      "+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|[[Normal_T0_0, Normal_T0_2, Normal_T0_2]]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                |\n",
      "|[[Normal_T0_0, Normal_T0_3, Normal_T0_1, Normal_T0_1, Normal_T0_1, Normal_T0_2, Normal_T0_3, Normal_T0_2, Normal_T0_1, Normal_T0_2, Normal_T0_2, Normal_T0_1, Normal_T0_2, Normal_T0_1, Normal_T0_3, Normal_T0_2, Normal_T0_1, Normal_T0_2, Normal_T0_2, Normal_T0_3, Normal_T0_2, Normal_T0_2, QuasiVuota_T0_3, Normal_T0_1, Normal_T0_1, Normal_T0_1, Normal_T0_2, QuasiVuota_T0_3, Normal_T0_2, Normal_T0_1, Normal_T0_3, Normal_T0_1, QuasiVuota_T0_3, Normal_T0_1], [QuasiVuota_T1_3, Normal_T1_3, QuasiVuota_T1_1, QuasiVuota_T1_3, Normal_T1_1, Normal_T1_3, QuasiVuota_T1_3], [Normal_T2_3, QuasiVuota_T2_1, QuasiVuota_T2_3, Normal_T2_3, Normal_T2_3], [QuasiVuota_T3_3, Normal_T3_3, Normal_T3_3], [Normal_T4_3]]             |\n",
      "|[[Normal_T0_3, Normal_T0_0, Normal_T0_1, Normal_T0_3, Normal_T0_3, Normal_T0_3, Normal_T0_3, Normal_T0_3, Normal_T0_3, Normal_T0_2, Normal_T0_2, Normal_T0_2, Normal_T0_3, Normal_T0_2, Normal_T0_3, Normal_T0_1, Normal_T0_2, Normal_T0_3, Normal_T0_3, Normal_T0_2, Normal_T0_1, Normal_T0_2, Normal_T0_3, QuasiVuota_T0_1, Normal_T0_3, Normal_T0_3, Normal_T0_3, Normal_T0_3, QuasiVuota_T0_3, Normal_T0_2, Normal_T0_3, Normal_T0_2, Normal_T0_3, QuasiVuota_T0_1], [QuasiVuota_T1_3, Normal_T1_1, QuasiVuota_T1_3, QuasiVuota_T1_1, Normal_T1_3, Normal_T1_3, QuasiVuota_T1_1], [Normal_T2_3, QuasiVuota_T2_3, QuasiVuota_T2_3, Normal_T2_3, Normal_T2_1], [QuasiVuota_T3_3, Normal_T3_3, Normal_T3_1], [Normal_T4_1]]             |\n",
      "|[[Normal_T0_1, Normal_T0_0, Normal_T0_3, Normal_T0_3, Normal_T0_3, Normal_T0_3, Normal_T0_3, Normal_T0_2, Normal_T0_2, Normal_T0_3, Normal_T0_3, Normal_T0_3, Normal_T0_1, Normal_T0_3, Normal_T0_3, Normal_T0_3, Normal_T0_2, Normal_T0_1, Normal_T0_2, QuasiVuota_T0_1, Normal_T0_3, Normal_T0_3, QuasiVuota_T0_3, Normal_T0_2, Normal_T0_3, Normal_T0_2, QuasiVuota_T0_1], [QuasiVuota_T1_3, Normal_T1_1, QuasiVuota_T1_3, QuasiVuota_T1_1, Normal_T1_3, Normal_T1_3, QuasiVuota_T1_1], [Normal_T2_3, QuasiVuota_T2_3, QuasiVuota_T2_3, Normal_T2_3, Normal_T2_1], [QuasiVuota_T3_3, Normal_T3_3, Normal_T3_1], [Normal_T4_1]]                                                                                                        |\n",
      "|[[Normal_T0_1, Normal_T0_3, Normal_T0_3, Normal_T0_0, Normal_T0_1, Normal_T0_1, Normal_T0_2, Normal_T0_3, Normal_T0_2, Normal_T0_1, Normal_T0_2, Normal_T0_2, Normal_T0_1, Normal_T0_1, Normal_T0_1, Normal_T0_3, Normal_T0_1, Normal_T0_1, Normal_T0_2, Normal_T0_1, Normal_T0_2, Normal_T0_2, Normal_T0_2, QuasiVuota_T0_3, Normal_T0_2, Normal_T0_1, Normal_T0_1, Normal_T0_2, QuasiVuota_T0_3, Normal_T0_1, Normal_T0_1, Normal_T0_3, Normal_T0_1, QuasiVuota_T0_2, Normal_T0_2], [QuasiVuota_T1_3, Normal_T1_2, QuasiVuota_T1_1, QuasiVuota_T1_3, Normal_T1_1, Normal_T1_3, QuasiVuota_T1_2], [Normal_T2_3, QuasiVuota_T2_1, QuasiVuota_T2_3, Normal_T2_3, Normal_T2_2], [QuasiVuota_T3_3, Normal_T3_3, Normal_T3_3], [Normal_T4_3]]|\n",
      "|[[Normal_T0_1, Normal_T0_3, Normal_T0_3, Normal_T0_1, Normal_T0_0, Normal_T0_1, Normal_T0_1, Normal_T0_2, Normal_T0_1, Normal_T0_1, Normal_T0_1, Normal_T0_2, Normal_T0_1, Normal_T0_1, Normal_T0_1, Normal_T0_3, Normal_T0_1, Normal_T0_1, Normal_T0_2, Normal_T0_1, Normal_T0_2, Normal_T0_2, Normal_T0_1, QuasiVuota_T0_3, Normal_T0_1, Normal_T0_1, Normal_T0_2, Normal_T0_2, QuasiVuota_T0_2, Normal_T0_2, Normal_T0_1, Normal_T0_3, Normal_T0_1, QuasiVuota_T0_3, Normal_T0_2], [QuasiVuota_T1_2, Normal_T1_3, QuasiVuota_T1_2, QuasiVuota_T1_3, Normal_T1_2, Normal_T1_2, QuasiVuota_T1_3], [Normal_T2_2, QuasiVuota_T2_2, QuasiVuota_T2_2, Normal_T2_2, Normal_T2_3], [QuasiVuota_T3_2, Normal_T3_2, Normal_T3_3], [Normal_T4_3]]|\n",
      "|[[Normal_T0_1, Normal_T0_3, Normal_T0_1, Normal_T0_1, Normal_T0_0, Normal_T0_1, Normal_T0_3, Normal_T0_2, Normal_T0_1, Normal_T0_2, Normal_T0_2, Normal_T0_1, Normal_T0_1, Normal_T0_1, Normal_T0_3, Normal_T0_2, Normal_T0_1, Normal_T0_2, Normal_T0_2, Normal_T0_3, Normal_T0_2, Normal_T0_2, QuasiVuota_T0_3, Normal_T0_1, Normal_T0_1, Normal_T0_2, Normal_T0_2, QuasiVuota_T0_3, Normal_T0_2, Normal_T0_1, Normal_T0_3, Normal_T0_1, QuasiVuota_T0_3, Normal_T0_2], [QuasiVuota_T1_3, Normal_T1_3, QuasiVuota_T1_2, QuasiVuota_T1_3, Normal_T1_2, Normal_T1_3, QuasiVuota_T1_3], [Normal_T2_3, QuasiVuota_T2_2, QuasiVuota_T2_3, Normal_T2_3, Normal_T2_3], [QuasiVuota_T3_3, Normal_T3_3, Normal_T3_3], [Normal_T4_3]]             |\n",
      "|[[Normal_T0_0, Normal_T0_1, Normal_T0_1, Normal_T0_3, QuasiVuota_T0_1, QuasiVuota_T0_2, QuasiVuota_T0_1, Normal_T0_1, Normal_T0_2, Normal_T0_1, QuasiVuota_T0_1, Normal_T0_1, QuasiVuota_T0_1, Normal_T0_1, QuasiVuota_T0_2], [Normal_T1_1, Normal_T1_1, Normal_T1_1, QuasiVuota_T1_1, QuasiVuota_T1_1], [Normal_T2_2, Normal_T2_1, Normal_T2_1], [QuasiVuota_T3_2, Normal_T3_1, Normal_T3_2], [QuasiVuota_T4_2]]                                                                                                                                                                                                                                                                                                                        |\n",
      "|[[Normal_T0_1, Normal_T0_0, Normal_T0_1, Normal_T0_2, QuasiVuota_T0_1, QuasiVuota_T0_1, QuasiVuota_T0_1, Normal_T0_1, Normal_T0_2, Normal_T0_1, QuasiVuota_T0_1, Normal_T0_1, QuasiVuota_T0_1, Normal_T0_1, QuasiVuota_T0_2], [Normal_T1_1, Normal_T1_1, Normal_T1_1, QuasiVuota_T1_1, QuasiVuota_T1_1], [Normal_T2_2, Normal_T2_1, Normal_T2_1], [QuasiVuota_T3_2, Normal_T3_1, Normal_T3_2], [QuasiVuota_T4_2]]                                                                                                                                                                                                                                                                                                                        |\n",
      "|[[Normal_T0_0, Normal_T0_3, Normal_T0_2, Normal_T0_2]]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   |\n",
      "|[[Normal_T0_2, Normal_T0_3, Normal_T0_2, Normal_T0_1, Normal_T0_1, Normal_T0_0, Normal_T0_2, Normal_T0_1, Normal_T0_1, Normal_T0_2, Normal_T0_2, Normal_T0_2, Normal_T0_1, Normal_T0_1, Normal_T0_3, Normal_T0_2, Normal_T0_1, Normal_T0_2, Normal_T0_2, Normal_T0_3, Normal_T0_2, Normal_T0_1, QuasiVuota_T0_3, Normal_T0_1, Normal_T0_2, Normal_T0_2, Normal_T0_1, QuasiVuota_T0_2, Normal_T0_2, Normal_T0_1, Normal_T0_3, Normal_T0_1, QuasiVuota_T0_3, Normal_T0_2], [QuasiVuota_T1_2, Normal_T1_3, QuasiVuota_T1_2, QuasiVuota_T1_3, Normal_T1_2, Normal_T1_2, QuasiVuota_T1_3], [Normal_T2_2, QuasiVuota_T2_2, QuasiVuota_T2_2, Normal_T2_2, Normal_T2_3], [QuasiVuota_T3_2, Normal_T3_2, Normal_T3_3], [Normal_T4_3]]             |\n",
      "|[[Normal_T0_0, Normal_T0_1, Normal_T0_1, Normal_T0_3, Normal_T0_1, Normal_T0_1]]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         |\n",
      "|[[Normal_T0_3, Normal_T0_3, Normal_T0_3, Normal_T0_3, Normal_T0_2, Normal_T0_3, Normal_T0_2, Normal_T0_0, Normal_T0_1, Normal_T0_2, Normal_T0_2, Normal_T0_2, Normal_T0_3, Normal_T0_2, Normal_T0_2, Normal_T0_2, Normal_T0_2, Normal_T0_3, Normal_T0_1, Normal_T0_2, Normal_T0_2, Normal_T0_2, Normal_T0_2, QuasiVuota_T0_2, Normal_T0_2, Normal_T0_3, Normal_T0_3, Normal_T0_1, QuasiVuota_T0_1, Normal_T0_2, Normal_T0_2, Normal_T0_1, Normal_T0_2, QuasiVuota_T0_3], [QuasiVuota_T1_0, Normal_T1_3, QuasiVuota_T1_3, QuasiVuota_T1_2, Normal_T1_3, Normal_T1_0, QuasiVuota_T1_3], [Normal_T2_1, QuasiVuota_T2_3, QuasiVuota_T2_0, Normal_T2_0, Normal_T2_3], [QuasiVuota_T3_0, Normal_T3_1, Normal_T3_2], [Normal_T4_2]]             |\n",
      "|[[Normal_T0_1, Normal_T0_0, Normal_T0_1, Normal_T0_3, Normal_T0_1, Normal_T0_1]]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         |\n",
      "|[[Normal_T0_2, Normal_T0_3, Normal_T0_3, Normal_T0_2, Normal_T0_1, Normal_T0_2, Normal_T0_1, Normal_T0_1, Normal_T0_0, Normal_T0_1, Normal_T0_1, Normal_T0_1, Normal_T0_2, Normal_T0_1, Normal_T0_1, Normal_T0_3, Normal_T0_1, Normal_T0_2, Normal_T0_1, Normal_T0_1, Normal_T0_2, Normal_T0_2, Normal_T0_1, QuasiVuota_T0_3, Normal_T0_1, Normal_T0_2, Normal_T0_2, Normal_T0_1, QuasiVuota_T0_1, Normal_T0_2, Normal_T0_1, Normal_T0_2, Normal_T0_1, QuasiVuota_T0_3, Normal_T0_3], [QuasiVuota_T1_1, Normal_T1_3, QuasiVuota_T1_2, QuasiVuota_T1_3, Normal_T1_2, Normal_T1_1, QuasiVuota_T1_3], [Normal_T2_1, QuasiVuota_T2_2, QuasiVuota_T2_1, Normal_T2_1, Normal_T2_3], [QuasiVuota_T3_1, Normal_T3_1, Normal_T3_3], [Normal_T4_3]]|\n",
      "|[[Normal_T0_1, Normal_T0_1, Normal_T0_0, Normal_T0_2, QuasiVuota_T0_1, QuasiVuota_T0_1, QuasiVuota_T0_1, Normal_T0_1, Normal_T0_2, Normal_T0_1, QuasiVuota_T0_1, Normal_T0_1, QuasiVuota_T0_1, Normal_T0_1, QuasiVuota_T0_2], [Normal_T1_1, Normal_T1_1, Normal_T1_1, QuasiVuota_T1_1, QuasiVuota_T1_1], [Normal_T2_2, Normal_T2_1, Normal_T2_1], [QuasiVuota_T3_2, Normal_T3_1, Normal_T3_2], [QuasiVuota_T4_2]]                                                                                                                                                                                                                                                                                                                        |\n",
      "|[[Normal_T0_2, Normal_T0_3, Normal_T0_0, Normal_T0_1]]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   |\n",
      "|[[Normal_T0_3, Normal_T0_2, Normal_T0_2, Normal_T0_0, QuasiVuota_T0_2, QuasiVuota_T0_3, QuasiVuota_T0_2, Normal_T0_2, Normal_T0_2, Normal_T0_3, QuasiVuota_T0_2, Normal_T0_2, QuasiVuota_T0_2, Normal_T0_3, QuasiVuota_T0_3], [Normal_T1_2, Normal_T1_2, Normal_T1_2, QuasiVuota_T1_2, QuasiVuota_T1_2], [Normal_T2_3, Normal_T2_2, Normal_T2_2], [QuasiVuota_T3_3, Normal_T3_2, Normal_T3_3], [QuasiVuota_T4_3]]                                                                                                                                                                                                                                                                                                                        |\n",
      "|[[Normal_T0_0, Normal_T0_1, Normal_T0_1, Normal_T0_3]]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   |\n",
      "|[[Normal_T0_1, Normal_T0_2, Normal_T0_3, Normal_T0_1, Normal_T0_1, Normal_T0_1, Normal_T0_1, Normal_T0_2, Normal_T0_1, Normal_T0_0, Normal_T0_1, Normal_T0_1, Normal_T0_1, Normal_T0_1, Normal_T0_1, Normal_T0_3, Normal_T0_1, Normal_T0_1, Normal_T0_2, Normal_T0_1, Normal_T0_2, Normal_T0_1, Normal_T0_2, QuasiVuota_T0_3, Normal_T0_1, Normal_T0_2, Normal_T0_2, Normal_T0_2, QuasiVuota_T0_2, Normal_T0_1, Normal_T0_1, Normal_T0_3, Normal_T0_1, QuasiVuota_T0_2, Normal_T0_2], [QuasiVuota_T1_2, Normal_T1_2, QuasiVuota_T1_2, QuasiVuota_T1_3, Normal_T1_2, Normal_T1_2, QuasiVuota_T1_2], [Normal_T2_2, QuasiVuota_T2_2, QuasiVuota_T2_2, Normal_T2_2, Normal_T2_2], [QuasiVuota_T3_2, Normal_T3_2, Normal_T3_3], [Normal_T4_3]]|\n",
      "+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(20,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fleet-french",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.05\n"
     ]
    }
   ],
   "source": [
    "#prefixspan to obtain sequence and frequence\n",
    "counter=[]\n",
    "for support in supports:\n",
    "    print(support)\n",
    "    prefixSpan = PrefixSpan(minSupport=support, maxPatternLength=5,\n",
    "                        maxLocalProjDBSize=5000)\n",
    "    prefix=prefixSpan.findFrequentSequentialPatterns(df)   \n",
    "    len_prefix=prefix.count()    \n",
    "#     prefix.show(len_prefix,False)\n",
    "    counter.append(len_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unauthorized-distance",
   "metadata": {},
   "outputs": [],
   "source": [
    "len_prefix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "capital-tsunami",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre=prefix.rdd.map(tuple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "charitable-prototype",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre.collect()\n",
    "\n",
    "# [([['QuasiPiena_T0_1']], 22981),\n",
    "#  ([['Piena_T1_1']], 9208),\n",
    "#  ([['QuasiPiena_T2_1']], 21641),\n",
    "#  ([['Piena_T2_2']], 12794),\n",
    "#  ([['QuasiPiena_T2_3']], 18047),\n",
    "#  ([['Piena_T2_3']], 6994),\n",
    "#  ([['Piena_T1_0']], 17930),\n",
    "#  ([['QuasiPiena_T0_3']], 18099), ... ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "metropolitan-mattress",
   "metadata": {},
   "outputs": [],
   "source": [
    "def giveSelected(line):\n",
    "    seq=line[0]\n",
    "    found=False   \n",
    "    for window in seq:\n",
    "        for el in window:\n",
    "            if 'T0' in el and '_0' in el:\n",
    "                found=True\n",
    "                break\n",
    "    return found "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dressed-newspaper",
   "metadata": {},
   "outputs": [],
   "source": [
    "giveT0=pre.filter(giveSelected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "informal-fusion",
   "metadata": {},
   "outputs": [],
   "source": [
    "giveT0.collect()\n",
    "\n",
    "#CONFIGURAZIONE 30min, 1000m, 0.02supp, (3,3)\n",
    "# [([['Normal_T0_0']], 98781),\n",
    "#  ([['QuasiVuota_T0_0']], 96186),\n",
    "#  ([['Normal_T0_0'], ['QuasiVuota_T1_0']], 37443),\n",
    "#  ([['QuasiVuota_T0_0'], ['Normal_T1_0']], 57230), ..]\n",
    " \n",
    "#CONFIGURAZIONE 30min, 1000m, 0.02supp, (4,3)\n",
    "# [([['Normal_T0_0']], 98843),\n",
    "#  ([['QuasiVuota_T0_0']], 96162),\n",
    "#  ([['Normal_T0_0'], ['QuasiVuota_T1_0']], 37502),\n",
    "#  ([['QuasiVuota_T0_0'], ['Normal_T1_0']], 57643), ... ]\n",
    "\n",
    "#CONFIGURAZIONE 30min, 1000m, 0.02supp, (5,3)\n",
    "# in corso...\n",
    "\n",
    "\n",
    "#CONFIGURAZIONE 15min, 1000m, 0.05supp, (3,3)\n",
    "# [([['Normal_T0_0']], 108534),\n",
    "#  ([['QuasiVuota_T0_0']], 106732),\n",
    "#  ([['QuasiVuota_T0_0'], ['Normal_T1_0']], 52038),\n",
    "#  ([['Normal_T0_0'], ['QuasiVuota_T1_0']], 38927), ...]\n",
    "\n",
    "#CONFIGURAZIONE 15min, 1000m, 0.05supp, (4,3)\n",
    "# [([['Normal_T0_0']], 108437),\n",
    "#  ([['QuasiVuota_T0_0']], 106922),\n",
    "#  ([['Normal_T0_0'], ['QuasiVuota_T1_0']], 38816),\n",
    "#  ([['QuasiVuota_T0_0'], ['Normal_T1_0']], 51954),...]\n",
    " \n",
    "#CONFIGURAZIONE 15min, 1000m, 0.05supp, (5,3)\n",
    "# [([['Normal_T0_0']], 108406),\n",
    "#  ([['QuasiVuota_T0_0']], 106735),\n",
    "#  ([['QuasiVuota_T0_0'], ['Normal_T1_0']], 52070),\n",
    "#  ([['Normal_T0_0'], ['QuasiVuota_T1_0']], 38595), ...]\n",
    "\n",
    "\n",
    "#CONFIGURAZIONE 15min, 500m, 0.01supp, (3,3)\n",
    "# [([['Normal_T0_0']], 108697),\n",
    "#  ([['QuasiVuota_T0_0']], 107143),\n",
    "#  ([['QuasiVuota_T0_0'], ['Normal_T1_0']], 52190),\n",
    "#  ([['Normal_T0_0'], ['QuasiVuota_T1_0']], 38994), ...]\n",
    " \n",
    "#CONFIGURAZIONE 15min, 500m, 0.02supp, (4,3)\n",
    "# [([['Normal_T0_0']], 108735),\n",
    "#  ([['QuasiVuota_T0_0']], 106970),\n",
    "#  ([['QuasiVuota_T0_0'], ['Normal_T1_0']], 52376),\n",
    "#  ([['Normal_T0_0'], ['QuasiVuota_T1_0']], 38905), ...]\n",
    "\n",
    "#CONFIGURAZIONE 15min, 500m, 0.02supp, (5,3)\n",
    "# [([['Normal_T0_0']], 108330),\n",
    "#  ([['QuasiVuota_T0_0']], 107018),\n",
    "#  ([['QuasiVuota_T0_0'], ['Normal_T1_0']], 52144),\n",
    "#  ([['Normal_T0_0'], ['QuasiVuota_T1_0']], 38716), ...]\n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incorporate-transparency",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2=giveT0.toDF().withColumnRenamed('_1','sequence')\n",
    "df2=df2.withColumnRenamed('_2','freq')#.show(len_prefix,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unknown-guarantee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "representative-afghanistan",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapValues(line):\n",
    "    seq=line[0]\n",
    "    final=''\n",
    "    #voc[seq]=line[1]\n",
    "    for i,window in enumerate(seq):\n",
    "        if i>0:\n",
    "            final+='-'\n",
    "        for j,el in enumerate(window):\n",
    "            if j>0:\n",
    "                final+=','\n",
    "            final+=el\n",
    "    final+=(';'+str(line[1])+';'+str(i))\n",
    "    return final  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fatty-observation",
   "metadata": {},
   "outputs": [],
   "source": [
    "mapDict=giveT0.map(mapValues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "numerous-attraction",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mapDict.collect()\n",
    "\n",
    "# ['QuasiVuota_T0_0;282;0',\n",
    "#  'Vuota_T0_0;192;0',\n",
    "#  'QuasiVuota_T0_0,Vuota_T0_3;2;0',\n",
    "#  'QuasiVuota_T0_0,Vuota_T0_3-QuasiVuota_T1_1;1;1',\n",
    "#  'QuasiVuota_T0_0,Vuota_T0_3-QuasiVuota_T1_1,QuasiVuota_T1_3;1;1', ...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "occupational-foundation",
   "metadata": {},
   "outputs": [],
   "source": [
    "li=mapDict.collect()\n",
    "voc={}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "consolidated-treasury",
   "metadata": {},
   "outputs": [],
   "source": [
    "for el in li:\n",
    "    splits=el.split(';')\n",
    "    voc[splits[0]]=int(splits[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "handled-invitation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# voc\n",
    "\n",
    "# {'QuasiVuota_T0_0': 88948,\n",
    "#  'Vuota_T0_0': 44987,\n",
    "#  'Vuota_T0_0-QuasiVuota_T1_2': 33837,\n",
    "#  'QuasiVuota_T0_0-Vuota_T1_0': 26011,\n",
    "#  'QuasiVuota_T0_2,Vuota_T0_0': 34141, ...}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "experienced-locator",
   "metadata": {},
   "outputs": [],
   "source": [
    "repeated_el_window=0\n",
    "for el in voc.keys():\n",
    "    flag_rep=False\n",
    "    windows=el.split('-')\n",
    "    for w in windows:\n",
    "        tot_items=len(w.split(','))\n",
    "        set_items=len(set(w.split(',')))\n",
    "        if tot_items!=set_items:\n",
    "            repeated_el_window+=1\n",
    "            break\n",
    "repeated_el_window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incident-photographer",
   "metadata": {},
   "outputs": [],
   "source": [
    "voc_supports={}\n",
    "for el in voc.keys():    \n",
    "    if len(el.split('-'))>1:        \n",
    "        num=int(voc[el])       \n",
    "        string=''\n",
    "        tot=el.split('-')[:-1]\n",
    "        for k,station in enumerate(tot):\n",
    "            if k>0:\n",
    "                string+='-'\n",
    "            string+=station\n",
    "        #print(string)\n",
    "        den=int(voc[string])\n",
    "        voc_supports[el]=str(num/den)+' - '+str(voc[el])\n",
    "keys=list(voc_supports.keys())\n",
    "values=list(voc_supports.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eastern-polymer",
   "metadata": {},
   "outputs": [],
   "source": [
    "# voc_supports\n",
    "\n",
    "# {'Vuota_T0_0-QuasiVuota_T1_2': '0.7521506212905951 - 33837',\n",
    "#  'QuasiVuota_T0_0-Vuota_T1_0': '0.29242928452579037 - 26011',\n",
    "#  'Vuota_T0_0-QuasiVuota_T2_3': '0.4979438504456843 - 22401',\n",
    "#  'Vuota_T0_0-Vuota_T2_3': '0.28939471402849715 - 13019',\n",
    "#  'QuasiVuota_T0_0-QuasiVuota_T2_0': '0.3469780096236003 - 30863', ...}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "static-visibility",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sort vocabulary by decreasing values and sort within each window\n",
    "tot_frequence=0\n",
    "for key in voc_supports:    \n",
    "    splitted=key.split('-')      \n",
    "    splitted.sort()\n",
    "    tmp_frequence=int(voc_supports[key].split(' - ')[1])\n",
    "    tot_frequence+=tmp_frequence\n",
    "for key in voc_supports:\n",
    "    freq= int(voc_supports[key].split(' - ')[1])\n",
    "    #print(freq)\n",
    "#     perc_value=float(\"{:.2f}\".format(freq*100/tot_frequence))\n",
    "    voc_supports[key]=str(voc_supports[key].split(' - ')[0])+' - ' + str(freq)#+' - '+str(perc_value)+'%'  \n",
    "# voc_supports = dict(sorted(voc_supports.items(), key=operator.itemgetter(1),reverse=True))\n",
    "voc_supports = dict(sorted(voc_supports.items(), \n",
    "                           key=lambda v: (float(v[1].split(' - ')[0]), int(v[1].split(' - ')[1])),\n",
    "                           reverse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "curious-literature",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(output_file, \"w\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "binary-island",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_pattern=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wrapped-partition",
   "metadata": {},
   "outputs": [],
   "source": [
    "for el in voc_supports:\n",
    "    key_list=[]\n",
    "    for e in el.split('-'):\n",
    "        key_list.append([e])\n",
    "    list_pattern.append([[key_list], [voc_supports[el]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "upper-arrest",
   "metadata": {},
   "outputs": [],
   "source": [
    "file.write('Pattern, Confidence-Frequence'+'\\n')\n",
    "file.write(f'Total number of input patterns: {len(voc_supports)}'+'\\n')\n",
    "for el in list_pattern:  \n",
    "    file.write(str(el)+ '\\n')    \n",
    "file.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "statewide-centre",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # delete supp\n",
    "# for key in voc_supports:\n",
    "#     v=voc_supports[key].split('-')\n",
    "#     voc_supports[key]=v[0]+' - '+v[1]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "designing-coaching",
   "metadata": {},
   "outputs": [],
   "source": [
    "# voc_supports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "focal-johnson",
   "metadata": {},
   "outputs": [],
   "source": [
    "# output_file4='results_1000_0_ordered_by_support.txt'\n",
    "file4 = open(output_file4, \"w\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "linear-rhythm",
   "metadata": {},
   "outputs": [],
   "source": [
    "#order list fist by support and then by confidence\n",
    "list_ordered_by_support = sorted(list_pattern,\n",
    "                                 key=lambda v: (int(v[1][0].split(\" - \")[1]), float(v[1][0].split(\" - \")[0])),\n",
    "                                 reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baking-forth",
   "metadata": {},
   "outputs": [],
   "source": [
    "file4.write('Pattern, Confidence-Frequence'+'\\n')\n",
    "file4.write(f'Total number of input patterns: {len(voc_supports)}'+'\\n')\n",
    "for el in list_ordered_by_support:  \n",
    "    file4.write(str(el)+ '\\n')    \n",
    "file4.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "speaking-surname",
   "metadata": {},
   "outputs": [],
   "source": [
    "tot_list=[]\n",
    "for i, j in zip(keys,values):\n",
    "    key_list=[]\n",
    "    for el in i.split('-'):\n",
    "        key_list.append([el])\n",
    "    tot_list.append([[key_list], [j]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "expressed-summary",
   "metadata": {},
   "outputs": [],
   "source": [
    "confidence=[]\n",
    "for el in values:\n",
    "    #print(el.split('-')[0])\n",
    "    val=round(float(el.split(' - ')[0]),2)\n",
    "    confidence.append(val)\n",
    "#confidence    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rubber-effects",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(confidence)\n",
    "plt.xlabel('Confidence')\n",
    "plt.ylabel('Number of patterns')\n",
    "plt.title(f'{maxDelta} threshold spaziale, {window_size} threshold temporale {th*1000} m e supporto {support}')\n",
    "plt.savefig(img_support)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fourth-thumb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #how many min, max, avg items there are\n",
    "# num_items=0\n",
    "# num_freq_items=0\n",
    "# min_items=sys.maxsize\n",
    "# max_items=0\n",
    "# avg_items=0\n",
    "\n",
    "# num_freq_window=0\n",
    "# tot_window=0\n",
    "# num_patterns=0\n",
    "# min_window=sys.maxsize\n",
    "# max_window=0\n",
    "# avg_window=0\n",
    "# flag_vuota=False\n",
    "# flag_quasi_vuota=False\n",
    "\n",
    "# list_vuota_quasi_vuota=[]\n",
    "\n",
    "# for key in voc_supports.keys():\n",
    "#     flag_vuota=False\n",
    "#     flag_quasi_vuota=False    \n",
    "#     string=''\n",
    "    \n",
    "#     #statistics for the windows\n",
    "#     if '-' in key:\n",
    "#         patterns=len(key.split('-'))\n",
    "#         tot_window+=patterns      \n",
    "#     else:\n",
    "#         tot_window+=1\n",
    "#         patterns=1\n",
    "#     num_patterns+=1\n",
    "    \n",
    "#     #statistics for the ìtems\n",
    "#     if ('-' in key and ',' in key):\n",
    "#         new_key=key       \n",
    "#         new_key=new_key.replace(',', '-')               \n",
    "#         items=new_key.split('-')\n",
    "#         for el in items:\n",
    "#             string+=el.split('_')[0]\n",
    "#             if el.split('_')[0].startswith('Vuota') and flag_vuota==False:\n",
    "#                 flag_vuota=True\n",
    "#             elif el.split('_')[0].startswith('QuasiVuota') and flag_quasi_vuota==False:\n",
    "#                 flag_quasi_vuota=True\n",
    "      \n",
    "       \n",
    "#     elif ('-' in key):\n",
    "#         items=key.split('-')\n",
    "#         for el in items:\n",
    "#             string+=el.split('_')[0]\n",
    "#             if el.split('_')[0].startswith('Vuota') and flag_vuota==False:\n",
    "#                 flag_vuota=True\n",
    "#             elif el.split('_')[0].startswith('QuasiVuota') and flag_quasi_vuota==False:\n",
    "#                 flag_quasi_vuota=True\n",
    "       \n",
    "#     elif ( ',' in key):\n",
    "#         items=key.split(',')\n",
    "#         for el in items:\n",
    "#             string+=el.split('_')[0]\n",
    "#             if el.split('_')[0].startswith('Vuota') and flag_vuota==False:\n",
    "#                 flag_vuota=True\n",
    "#             elif el.split('_')[0].startswith('QuasiVuota') and flag_quasi_vuota==False:\n",
    "#                 flag_quasi_vuota=True\n",
    "      \n",
    "#     else:\n",
    "#         items=list(key)\n",
    "#         for el in items:\n",
    "#             string+=el.split('_')[0]\n",
    "#             if el.split('_')[0].startswith('Vuota') and flag_vuota==False:\n",
    "#                 flag_vuota=True\n",
    "#             elif el.split('_')[0].startswith('QuasiVuota') and flag_quasi_vuota==False:\n",
    "#                 flag_quasi_vuota=True\n",
    "#     if (flag_quasi_vuota==True and flag_vuota==True): \n",
    "#         key_list=[]            \n",
    "#         for e in key.split('-'):\n",
    "#             key_list.append([e])\n",
    "#         list_vuota_quasi_vuota.append([[key_list], [voc_supports[key]]])\n",
    "\n",
    "#     q_items=len(items) \n",
    "#     if min_items>q_items:\n",
    "#         min_items=q_items\n",
    "#     if max_items<q_items:\n",
    "#         max_items=q_items\n",
    "#     freq=int(voc_supports[key].split('-')[1])\n",
    "#     num_items+=freq \n",
    "#     num_freq_items+=freq*q_items\n",
    "    \n",
    "#     if min_window> patterns:\n",
    "#         min_window=patterns\n",
    "#     if max_window<patterns:\n",
    "#         max_window=patterns    \n",
    "#     num_freq_window+=freq*patterns  \n",
    "\n",
    "# avg_window=0\n",
    "# if num_items!=0:\n",
    "#     avg_items=float(num_freq_items)/float(num_items)    \n",
    "#     print(f'The average number of items is: {avg_items}')    \n",
    "#     avg_window=float(num_freq_window)/ float(num_items)\n",
    "#     print(f'The average number of windows is: {avg_window}')\n",
    "# else:\n",
    "#     print(f'The average number of windows is: {avg_window}')\n",
    "\n",
    "# if min_items>1000:\n",
    "#     min_items=0\n",
    "#     min_window=0\n",
    "# print(f'The minimum number of items is: {min_items}')\n",
    "# print(f'The maximum number of items is: {max_items}')\n",
    "# print(f'The minimum number of windows is: {min_window}')\n",
    "# print(f'The maximum number of windows is: {max_window}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "steady-representation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('FILTER PATTERNS WITH AT LEAST 1 EVENT QUASI VUOTA AND 1 EVENT VUOTA ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecological-neighbor",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # output_file2='results_1000_0_QuasiVuota_Vuota_ordered_by_confidence.txt'\n",
    "# file2 = open(output_file2, \"w\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "polar-suggestion",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lung_vuota_quasivuota=0\n",
    "# if len(list_vuota_quasi_vuota)!=0:\n",
    "#     df_supports=sc.parallelize(list_vuota_quasi_vuota).toDF().withColumnRenamed('_1','sequence')\n",
    "#     df_supports=df_supports.withColumnRenamed('_2','confidence-freq')\n",
    "#     lung_vuota_quasivuota=df_supports.count() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "latest-inside",
   "metadata": {},
   "outputs": [],
   "source": [
    "# file2.write('Pattern, Confidence-Frequence'+'\\n')\n",
    "# file2.write(f'Total number of input patterns: {len(list_vuota_quasi_vuota)}'+'\\n')\n",
    "# if len(list_vuota_quasi_vuota)!=0:\n",
    "#     for el in list_vuota_quasi_vuota:\n",
    "#         #print(el)\n",
    "#         file2.write(str(el)+'\\n')\n",
    "# file2.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "developing-denmark",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # output_file5='results_1000_0_QuasiVuota_Vuota_ordered_by_support.txt'\n",
    "# file5 = open(output_file5, \"w\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spoken-douglas",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #order list fist by support an then by confidence\n",
    "# list_vuota_quasi_vuota_ordered_by_support = sorted(list_vuota_quasi_vuota,\n",
    "#                                                    key=lambda v: (int(v[1][0].split(\" - \")[1]), float(v[1][0].split(\" - \")[0])),\n",
    "#                                                    reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "elementary-immune",
   "metadata": {},
   "outputs": [],
   "source": [
    "# file5.write('Pattern, Confidence-Frequence'+'\\n')\n",
    "# file5.write(f'Total number of input patterns: {len(list_vuota_quasi_vuota_ordered_by_support)}'+'\\n')\n",
    "# if len(list_vuota_quasi_vuota_ordered_by_support)!=0:\n",
    "#     for el in list_vuota_quasi_vuota_ordered_by_support:\n",
    "#         #print(el)\n",
    "#         file5.write(str(el)+'\\n')\n",
    "# file5.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "listed-karma",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('FILTER PATTERNS WITH AT LEAST 1 T0, DELTA S=0 AND AT LEAST 1 PATTERN WITH AT LEAST 1 PATTERN WITH DELTA S DIFFERENT FROM 0 AND DELTA T DIFFERENT FROM 0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "representative-bearing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list_influenze=[]\n",
    "# for el in voc_supports.keys():\n",
    "#     delta_spaziale=False\n",
    "#     if ('-' in el):\n",
    "#         all_windows_list=el.split('-')\n",
    "#         if ('T0_0' in all_windows_list[0] ):\n",
    "#             for cons_window in all_windows_list[1::]:\n",
    "#                 if ',' in cons_window:\n",
    "#                     for item in cons_window.split(','):                       \n",
    "#                         act_delta=int(item.split('_')[2])\n",
    "#                         if act_delta!=0:\n",
    "#                             delta_spaziale=True\n",
    "#     if delta_spaziale==True:        \n",
    "#         key_list=[]            \n",
    "#         for e in el.split('-'):\n",
    "#             key_list.append([e])\n",
    "#         list_influenze.append([[key_list], [voc_supports[el]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hollywood-lover",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # output_file3='results_1000_0_diff_delta_ordered_by_confidence.txt'\n",
    "# file3 = open(output_file3, \"w\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "partial-renewal",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lung_different_time_space=0\n",
    "# if len(list_influenze)!=0:\n",
    "#     df_supports=sc.parallelize(list_influenze).toDF().withColumnRenamed('_1','sequence')\n",
    "#     df_supports=df_supports.withColumnRenamed('_2','confidence-freq')\n",
    "#     lung_different_time_space=df_supports.count() \n",
    "#     #df_supports.show(lung_different_time_space,False)\n",
    "#     #print(lung_different_time_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fundamental-convenience",
   "metadata": {},
   "outputs": [],
   "source": [
    "# file3.write('Pattern, Confidence-Frequence'+'\\n')\n",
    "# file3.write(f'Total number of input patterns: {len(list_influenze)}'+'\\n')\n",
    "# if len(list_influenze)!=0:\n",
    "#     for el in list_influenze:       \n",
    "#         file3.write(str(el)+'\\n')\n",
    "# file3.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plain-raleigh",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # output_file6='results_1000_0_diff_delta_ordered_by_support.txt'\n",
    "# file6 = open(output_file6, \"w\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "israeli-difference",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #order list first by support and then by confidence\n",
    "# list_influenze_ordered_by_support = sorted(list_influenze,\n",
    "#                                            key=lambda v: (int(v[1][0].split(\" - \")[1]), float(v[1][0].split(\" - \")[0])),\n",
    "#                                            reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "light-fleece",
   "metadata": {},
   "outputs": [],
   "source": [
    "# file6.write('Pattern, Confidence-Frequence'+'\\n')\n",
    "# file6.write(f'Total number of input patterns: {len(list_influenze_ordered_by_support)}'+'\\n')\n",
    "# if len(list_influenze_ordered_by_support)!=0:\n",
    "#     for el in list_influenze_ordered_by_support:       \n",
    "#         file6.write(str(el)+'\\n')\n",
    "# file6.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "found-milwaukee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f'The number of patterns in the pre-filter is: {len_prefix}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "designed-summit",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f'The number of items after the filter with at least 2 windows and at least a T0 and delta 0 is: {len(voc_supports)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "third-alexander",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('STATISTICS about sequences with at least 2 windows_T0_delta0')\n",
    "# print(f'The average number of windows is: {avg_window}')\n",
    "# print(f'The minimum number of windows is: {min_window}')\n",
    "# print(f'The maximum number of windows is: {max_window}')\n",
    "# print(f'The average number of items is: {avg_items}')\n",
    "# print(f'The minimum number of items is: {min_items}')\n",
    "# print(f'The maximum number of items is: {max_items}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "promotional-danger",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f'The number of patterns in which there is at least one item that repeats within a window is: {repeated_el_window} ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "golden-mississippi",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f'The number of patterns with at least 1 event QuasiVuota and 1 event Vuota is: {lung_vuota_quasivuota}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "capital-reunion",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  print(f'The number of patterns with at least 1 T0, DELTA S=0 and at least 1 pattern with at least 1 pattern with DELTA S different from 0 and DELTA T different from 0 is: {lung_different_time_space}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "posted-yeast",
   "metadata": {},
   "outputs": [],
   "source": [
    "end = time.time()\n",
    "print(f'The time of execution is: {end-start} s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "greater-consent",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark (Yarn)",
   "language": "python",
   "name": "pyspark_yarn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
